{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet_ml import Experiment\n",
    "# experiment = Experiment(api_key=\"xktj4EX0zB8YcQ3BEaFwOQYpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\train\\\\\"\n",
    "train_csv = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\train\\\\ids.csv\"\n",
    "test_path = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\test\\\\\"\n",
    "test_csv = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\test\\\\ids.csv\"\n",
    "\n",
    "dest_path = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\\"\n",
    "\n",
    "csv = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\ids.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv,index_col=0).sample(frac=1)\n",
    "\n",
    "df['log_volume'] = df['volume'].apply(np.log)\n",
    "\n",
    "x = df.drop(columns=['name','date','flow_rate','source','moments','inertia_tensor'])\n",
    "x = x.as_matrix()\n",
    "# x = standardize = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>flow_rate</th>\n",
       "      <th>source</th>\n",
       "      <th>peak_val</th>\n",
       "      <th>area</th>\n",
       "      <th>volume</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>euler_number</th>\n",
       "      <th>extent</th>\n",
       "      <th>mean_intensity</th>\n",
       "      <th>moments</th>\n",
       "      <th>orientation</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>inertia_tensor</th>\n",
       "      <th>cluster</th>\n",
       "      <th>num_peaks</th>\n",
       "      <th>log_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>2017.11.22</td>\n",
       "      <td>3600</td>\n",
       "      <td>0.80 s.tifheightmap.mat</td>\n",
       "      <td>3.246003</td>\n",
       "      <td>2078</td>\n",
       "      <td>3636.158141</td>\n",
       "      <td>0.965494</td>\n",
       "      <td>1</td>\n",
       "      <td>0.546842</td>\n",
       "      <td>1.749835</td>\n",
       "      <td>[[2.07800000e+03 1.12295000e+05 7.50542500e+06...</td>\n",
       "      <td>-1.408393</td>\n",
       "      <td>250.551299</td>\n",
       "      <td>[[  65.34587122 -105.42900115]\\r\\n [-105.42900...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.198683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>93_flipped</td>\n",
       "      <td>2017.11.22</td>\n",
       "      <td>1800</td>\n",
       "      <td>6.80 s.tifheightmap.mat</td>\n",
       "      <td>5.070634</td>\n",
       "      <td>4676</td>\n",
       "      <td>11830.638042</td>\n",
       "      <td>0.974598</td>\n",
       "      <td>1</td>\n",
       "      <td>0.428205</td>\n",
       "      <td>2.530077</td>\n",
       "      <td>[[4.67600000e+03 3.91785000e+05 4.31880770e+07...</td>\n",
       "      <td>-1.425538</td>\n",
       "      <td>455.847763</td>\n",
       "      <td>[[ 158.40076381 -307.5813893 ]\\r\\n [-307.58138...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9.378448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>178</td>\n",
       "      <td>2017.11.23</td>\n",
       "      <td>3600</td>\n",
       "      <td>63.41 s.tifheightmap.mat</td>\n",
       "      <td>7.557547</td>\n",
       "      <td>1761</td>\n",
       "      <td>8003.187696</td>\n",
       "      <td>0.516807</td>\n",
       "      <td>1</td>\n",
       "      <td>0.626913</td>\n",
       "      <td>4.544684</td>\n",
       "      <td>[[1.76100000e+03 4.37900000e+04 1.36807200e+06...</td>\n",
       "      <td>1.081748</td>\n",
       "      <td>166.409163</td>\n",
       "      <td>[[133.39356704  18.65943251]\\r\\n [ 18.65943251...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.987595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>77_flipped</td>\n",
       "      <td>2017.11.22</td>\n",
       "      <td>1800</td>\n",
       "      <td>12.59 s.tifheightmap.mat</td>\n",
       "      <td>8.562641</td>\n",
       "      <td>3641</td>\n",
       "      <td>10439.409357</td>\n",
       "      <td>0.953193</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386450</td>\n",
       "      <td>2.864035</td>\n",
       "      <td>[[3.64500000e+03 2.50714000e+05 2.16705220e+07...</td>\n",
       "      <td>-1.184230</td>\n",
       "      <td>392.433550</td>\n",
       "      <td>[[ 307.51733073 -442.34137532]\\r\\n [-442.34137...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9.253343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>137</td>\n",
       "      <td>2017.11.23</td>\n",
       "      <td>1800</td>\n",
       "      <td>30.40 s.tifheightmap.mat</td>\n",
       "      <td>12.672682</td>\n",
       "      <td>7138</td>\n",
       "      <td>51565.232138</td>\n",
       "      <td>0.762010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.581065</td>\n",
       "      <td>7.211921</td>\n",
       "      <td>[[7.15000000e+03 3.91770000e+05 2.49082840e+07...</td>\n",
       "      <td>0.262741</td>\n",
       "      <td>435.847763</td>\n",
       "      <td>[[1008.79867512  152.90892718]\\r\\n [ 152.90892...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.850603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name        date  flow_rate                    source   peak_val  \\\n",
       "96           96  2017.11.22       3600   0.80 s.tifheightmap.mat   3.246003   \n",
       "395  93_flipped  2017.11.22       1800   6.80 s.tifheightmap.mat   5.070634   \n",
       "178         178  2017.11.23       3600  63.41 s.tifheightmap.mat   7.557547   \n",
       "377  77_flipped  2017.11.22       1800  12.59 s.tifheightmap.mat   8.562641   \n",
       "137         137  2017.11.23       1800  30.40 s.tifheightmap.mat  12.672682   \n",
       "\n",
       "     area        volume  eccentricity  euler_number    extent  mean_intensity  \\\n",
       "96   2078   3636.158141      0.965494             1  0.546842        1.749835   \n",
       "395  4676  11830.638042      0.974598             1  0.428205        2.530077   \n",
       "178  1761   8003.187696      0.516807             1  0.626913        4.544684   \n",
       "377  3641  10439.409357      0.953193             1  0.386450        2.864035   \n",
       "137  7138  51565.232138      0.762010             1  0.581065        7.211921   \n",
       "\n",
       "                                               moments  orientation  \\\n",
       "96   [[2.07800000e+03 1.12295000e+05 7.50542500e+06...    -1.408393   \n",
       "395  [[4.67600000e+03 3.91785000e+05 4.31880770e+07...    -1.425538   \n",
       "178  [[1.76100000e+03 4.37900000e+04 1.36807200e+06...     1.081748   \n",
       "377  [[3.64500000e+03 2.50714000e+05 2.16705220e+07...    -1.184230   \n",
       "137  [[7.15000000e+03 3.91770000e+05 2.49082840e+07...     0.262741   \n",
       "\n",
       "      perimeter                                     inertia_tensor  cluster  \\\n",
       "96   250.551299  [[  65.34587122 -105.42900115]\\r\\n [-105.42900...        1   \n",
       "395  455.847763  [[ 158.40076381 -307.5813893 ]\\r\\n [-307.58138...        2   \n",
       "178  166.409163  [[133.39356704  18.65943251]\\r\\n [ 18.65943251...        0   \n",
       "377  392.433550  [[ 307.51733073 -442.34137532]\\r\\n [-442.34137...        2   \n",
       "137  435.847763  [[1008.79867512  152.90892718]\\r\\n [ 152.90892...        0   \n",
       "\n",
       "     num_peaks  log_volume  \n",
       "96           1    8.198683  \n",
       "395          1    9.378448  \n",
       "178          1    8.987595  \n",
       "377          1    9.253343  \n",
       "137          1   10.850603  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(df.shape[0]/10)\n",
    "\n",
    "x_test = np.array(x[:test_size])\n",
    "y_test = df['flow_rate'][:test_size].values\n",
    "\n",
    "x_train = np.array(x[test_size:])\n",
    "y_train = df['flow_rate'][test_size:].values\n",
    "\n",
    "std = np.std(x_train,0)\n",
    "mean = np.mean(x_train,0)\n",
    "    \n",
    "x_train = (x_train-mean)/std\n",
    "x_test = (x_test-mean)/std        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0 ...   0   0 107]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 9], dtype=int64)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.bincount(y_train))\n",
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_classes = 3\n",
    "nb_epoch = 20\n",
    "\n",
    "lmda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train == 1800] = 0\n",
    "y_train[y_train == 3600] = 1\n",
    "y_train[y_train == 7200] = 2\n",
    "\n",
    "y_test[y_test == 1800] = 0\n",
    "y_test[y_test == 3600] = 1\n",
    "y_test[y_test == 7200] = 2\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes) \n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logistic_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim, input_dim=input_dim,\n",
    "                    kernel_regularizer=l2(lmda),\n",
    "                    activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model = build_logistic_model(input_dim, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_170 (Dense)            (None, 3)                 39        \n",
      "=================================================================\n",
      "Total params: 39\n",
      "Trainable params: 39\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 362 samples, validate on 40 samples\n",
      "Epoch 1/20\n",
      "362/362 [==============================] - 2s 6ms/step - loss: 1.5146 - acc: 0.3287 - val_loss: 1.3488 - val_acc: 0.3250\n",
      "Epoch 2/20\n",
      "362/362 [==============================] - 0s 312us/step - loss: 1.4301 - acc: 0.3315 - val_loss: 1.3154 - val_acc: 0.3250\n",
      "Epoch 3/20\n",
      "362/362 [==============================] - 0s 366us/step - loss: 1.3612 - acc: 0.3619 - val_loss: 1.2868 - val_acc: 0.3250\n",
      "Epoch 4/20\n",
      "362/362 [==============================] - 0s 257us/step - loss: 1.3078 - acc: 0.3646 - val_loss: 1.2673 - val_acc: 0.3500\n",
      "Epoch 5/20\n",
      "362/362 [==============================] - 0s 281us/step - loss: 1.2648 - acc: 0.3674 - val_loss: 1.2518 - val_acc: 0.3000\n",
      "Epoch 6/20\n",
      "362/362 [==============================] - 0s 272us/step - loss: 1.2326 - acc: 0.3702 - val_loss: 1.2384 - val_acc: 0.2750\n",
      "Epoch 7/20\n",
      "362/362 [==============================] - 0s 305us/step - loss: 1.2060 - acc: 0.3785 - val_loss: 1.2259 - val_acc: 0.2750\n",
      "Epoch 8/20\n",
      "362/362 [==============================] - 0s 301us/step - loss: 1.1855 - acc: 0.3757 - val_loss: 1.2125 - val_acc: 0.2750\n",
      "Epoch 9/20\n",
      "362/362 [==============================] - 0s 275us/step - loss: 1.1685 - acc: 0.3840 - val_loss: 1.2042 - val_acc: 0.2500\n",
      "Epoch 10/20\n",
      "362/362 [==============================] - 0s 285us/step - loss: 1.1536 - acc: 0.3729 - val_loss: 1.1928 - val_acc: 0.2500\n",
      "Epoch 11/20\n",
      "362/362 [==============================] - 0s 271us/step - loss: 1.1411 - acc: 0.3812 - val_loss: 1.1842 - val_acc: 0.2750\n",
      "Epoch 12/20\n",
      "362/362 [==============================] - 0s 286us/step - loss: 1.1307 - acc: 0.3785 - val_loss: 1.1743 - val_acc: 0.2500\n",
      "Epoch 13/20\n",
      "362/362 [==============================] - 0s 376us/step - loss: 1.1208 - acc: 0.3923 - val_loss: 1.1677 - val_acc: 0.3250\n",
      "Epoch 14/20\n",
      "362/362 [==============================] - 0s 379us/step - loss: 1.1120 - acc: 0.4144 - val_loss: 1.1614 - val_acc: 0.3250\n",
      "Epoch 15/20\n",
      "362/362 [==============================] - 0s 376us/step - loss: 1.1046 - acc: 0.4171 - val_loss: 1.1549 - val_acc: 0.3250\n",
      "Epoch 16/20\n",
      "362/362 [==============================] - 0s 386us/step - loss: 1.0977 - acc: 0.4227 - val_loss: 1.1502 - val_acc: 0.3250\n",
      "Epoch 17/20\n",
      "362/362 [==============================] - 0s 379us/step - loss: 1.0916 - acc: 0.4199 - val_loss: 1.1464 - val_acc: 0.3500\n",
      "Epoch 18/20\n",
      "362/362 [==============================] - 0s 451us/step - loss: 1.0870 - acc: 0.4199 - val_loss: 1.1424 - val_acc: 0.3500\n",
      "Epoch 19/20\n",
      "362/362 [==============================] - 0s 427us/step - loss: 1.0822 - acc: 0.4337 - val_loss: 1.1384 - val_acc: 0.3750\n",
      "Epoch 20/20\n",
      "362/362 [==============================] - 0s 423us/step - loss: 1.0782 - acc: 0.4309 - val_loss: 1.1351 - val_acc: 0.3500\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size, epochs=nb_epoch,\n",
    "                    verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 1.1350850105285644\n",
      "Test accuracy: 0.35\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 1 1 1 1 0 0 1 1 0 2 2 0 1 0 2 0 1 0 1 0 1 2 0 2 0 0 0 1 1 1 0 0 2 0\n",
      " 1 1 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(x_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test,axis = 1) \n",
    "\n",
    "print(Y_pred_classes)\n",
    "\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_classes = 3\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size, epochs=nb_epoch,\n",
    "                    verbose=1, validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 10\n",
      "Train on 360 samples, validate on 42 samples\n",
      "Epoch 1/20\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 1.2657 - acc: 0.3694 - val_loss: 1.3278 - val_acc: 0.3095\n",
      "Epoch 2/20\n",
      "360/360 [==============================] - 0s 392us/step - loss: 1.2360 - acc: 0.3750 - val_loss: 1.3077 - val_acc: 0.2857\n",
      "Epoch 3/20\n",
      "360/360 [==============================] - 0s 371us/step - loss: 1.2130 - acc: 0.3556 - val_loss: 1.2903 - val_acc: 0.2857\n",
      "Epoch 4/20\n",
      "360/360 [==============================] - 0s 424us/step - loss: 1.1940 - acc: 0.3639 - val_loss: 1.2759 - val_acc: 0.3095\n",
      "Epoch 5/20\n",
      "360/360 [==============================] - 0s 400us/step - loss: 1.1772 - acc: 0.3611 - val_loss: 1.2631 - val_acc: 0.3095\n",
      "Epoch 6/20\n",
      "360/360 [==============================] - 0s 379us/step - loss: 1.1645 - acc: 0.3694 - val_loss: 1.2515 - val_acc: 0.3095\n",
      "Epoch 7/20\n",
      "360/360 [==============================] - 0s 375us/step - loss: 1.1537 - acc: 0.3778 - val_loss: 1.2388 - val_acc: 0.3333\n",
      "Epoch 8/20\n",
      "360/360 [==============================] - 0s 370us/step - loss: 1.1434 - acc: 0.3944 - val_loss: 1.2294 - val_acc: 0.3095\n",
      "Epoch 9/20\n",
      "360/360 [==============================] - 0s 379us/step - loss: 1.1357 - acc: 0.3972 - val_loss: 1.2196 - val_acc: 0.3333\n",
      "Epoch 10/20\n",
      "360/360 [==============================] - 0s 370us/step - loss: 1.1287 - acc: 0.4139 - val_loss: 1.2109 - val_acc: 0.3571\n",
      "Epoch 11/20\n",
      "360/360 [==============================] - 0s 416us/step - loss: 1.1227 - acc: 0.4111 - val_loss: 1.2031 - val_acc: 0.3571\n",
      "Epoch 12/20\n",
      "360/360 [==============================] - 0s 368us/step - loss: 1.1169 - acc: 0.4167 - val_loss: 1.1951 - val_acc: 0.3571\n",
      "Epoch 13/20\n",
      "360/360 [==============================] - 0s 464us/step - loss: 1.1121 - acc: 0.4083 - val_loss: 1.1888 - val_acc: 0.3571\n",
      "Epoch 14/20\n",
      "360/360 [==============================] - 0s 485us/step - loss: 1.1073 - acc: 0.4111 - val_loss: 1.1830 - val_acc: 0.3333\n",
      "Epoch 15/20\n",
      "360/360 [==============================] - 0s 502us/step - loss: 1.1035 - acc: 0.4028 - val_loss: 1.1766 - val_acc: 0.3333\n",
      "Epoch 16/20\n",
      "360/360 [==============================] - 0s 653us/step - loss: 1.0998 - acc: 0.4028 - val_loss: 1.1708 - val_acc: 0.3333\n",
      "Epoch 17/20\n",
      "360/360 [==============================] - 0s 631us/step - loss: 1.0963 - acc: 0.4056 - val_loss: 1.1654 - val_acc: 0.2857\n",
      "Epoch 18/20\n",
      "360/360 [==============================] - 0s 468us/step - loss: 1.0933 - acc: 0.4056 - val_loss: 1.1611 - val_acc: 0.2857\n",
      "Epoch 19/20\n",
      "360/360 [==============================] - 0s 463us/step - loss: 1.0900 - acc: 0.4056 - val_loss: 1.1566 - val_acc: 0.2857\n",
      "Epoch 20/20\n",
      "360/360 [==============================] - 0s 484us/step - loss: 1.0872 - acc: 0.4028 - val_loss: 1.1527 - val_acc: 0.2857\n",
      "Running Fold 2 / 10\n",
      "Train on 360 samples, validate on 42 samples\n",
      "Epoch 1/20\n",
      "360/360 [==============================] - 3s 9ms/step - loss: 1.5686 - acc: 0.3417 - val_loss: 1.4044 - val_acc: 0.3810\n",
      "Epoch 2/20\n",
      "360/360 [==============================] - 0s 777us/step - loss: 1.4570 - acc: 0.3389 - val_loss: 1.3273 - val_acc: 0.4286\n",
      "Epoch 3/20\n",
      "360/360 [==============================] - 0s 486us/step - loss: 1.3736 - acc: 0.3306 - val_loss: 1.2706 - val_acc: 0.4286\n",
      "Epoch 4/20\n",
      "360/360 [==============================] - 0s 438us/step - loss: 1.3123 - acc: 0.3361 - val_loss: 1.2291 - val_acc: 0.4048\n",
      "Epoch 5/20\n",
      "360/360 [==============================] - 0s 527us/step - loss: 1.2644 - acc: 0.3333 - val_loss: 1.1980 - val_acc: 0.4524\n",
      "Epoch 6/20\n",
      "360/360 [==============================] - 0s 495us/step - loss: 1.2277 - acc: 0.3444 - val_loss: 1.1753 - val_acc: 0.4762\n",
      "Epoch 7/20\n",
      "360/360 [==============================] - 0s 429us/step - loss: 1.2009 - acc: 0.3361 - val_loss: 1.1590 - val_acc: 0.4762\n",
      "Epoch 8/20\n",
      "360/360 [==============================] - 0s 377us/step - loss: 1.1801 - acc: 0.3556 - val_loss: 1.1432 - val_acc: 0.4762\n",
      "Epoch 9/20\n",
      "360/360 [==============================] - 0s 407us/step - loss: 1.1642 - acc: 0.3556 - val_loss: 1.1347 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "360/360 [==============================] - 0s 411us/step - loss: 1.1522 - acc: 0.3611 - val_loss: 1.1257 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "360/360 [==============================] - 0s 374us/step - loss: 1.1419 - acc: 0.3778 - val_loss: 1.1213 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "360/360 [==============================] - 0s 371us/step - loss: 1.1331 - acc: 0.3750 - val_loss: 1.1150 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "360/360 [==============================] - 0s 393us/step - loss: 1.1257 - acc: 0.3833 - val_loss: 1.1122 - val_acc: 0.5238\n",
      "Epoch 14/20\n",
      "360/360 [==============================] - 0s 393us/step - loss: 1.1195 - acc: 0.3833 - val_loss: 1.1077 - val_acc: 0.5238\n",
      "Epoch 15/20\n",
      "360/360 [==============================] - 0s 434us/step - loss: 1.1144 - acc: 0.3944 - val_loss: 1.1043 - val_acc: 0.5238\n",
      "Epoch 16/20\n",
      "360/360 [==============================] - 0s 435us/step - loss: 1.1090 - acc: 0.3917 - val_loss: 1.1009 - val_acc: 0.5238\n",
      "Epoch 17/20\n",
      "360/360 [==============================] - 0s 429us/step - loss: 1.1050 - acc: 0.3944 - val_loss: 1.0986 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "360/360 [==============================] - 0s 413us/step - loss: 1.1005 - acc: 0.3917 - val_loss: 1.0956 - val_acc: 0.4762\n",
      "Epoch 19/20\n",
      "360/360 [==============================] - 0s 392us/step - loss: 1.0964 - acc: 0.3972 - val_loss: 1.0940 - val_acc: 0.4762\n",
      "Epoch 20/20\n",
      "360/360 [==============================] - 0s 382us/step - loss: 1.0935 - acc: 0.3917 - val_loss: 1.0929 - val_acc: 0.4524\n",
      "Running Fold 3 / 10\n",
      "Train on 361 samples, validate on 41 samples\n",
      "Epoch 1/20\n",
      "361/361 [==============================] - 3s 8ms/step - loss: 1.2126 - acc: 0.4183 - val_loss: 1.2970 - val_acc: 0.2927\n",
      "Epoch 2/20\n",
      "361/361 [==============================] - 0s 435us/step - loss: 1.1856 - acc: 0.4127 - val_loss: 1.2806 - val_acc: 0.2927\n",
      "Epoch 3/20\n",
      "361/361 [==============================] - 0s 352us/step - loss: 1.1644 - acc: 0.4072 - val_loss: 1.2658 - val_acc: 0.3171\n",
      "Epoch 4/20\n",
      "361/361 [==============================] - 0s 370us/step - loss: 1.1478 - acc: 0.4183 - val_loss: 1.2561 - val_acc: 0.3171\n",
      "Epoch 5/20\n",
      "361/361 [==============================] - 0s 356us/step - loss: 1.1345 - acc: 0.4266 - val_loss: 1.2478 - val_acc: 0.3171\n",
      "Epoch 6/20\n",
      "361/361 [==============================] - 0s 358us/step - loss: 1.1235 - acc: 0.4321 - val_loss: 1.2419 - val_acc: 0.3415\n",
      "Epoch 7/20\n",
      "361/361 [==============================] - 0s 384us/step - loss: 1.1151 - acc: 0.4294 - val_loss: 1.2372 - val_acc: 0.3171\n",
      "Epoch 8/20\n",
      "361/361 [==============================] - 0s 394us/step - loss: 1.1080 - acc: 0.4432 - val_loss: 1.2327 - val_acc: 0.3171\n",
      "Epoch 9/20\n",
      "361/361 [==============================] - 0s 371us/step - loss: 1.1022 - acc: 0.4432 - val_loss: 1.2285 - val_acc: 0.3171\n",
      "Epoch 10/20\n",
      "361/361 [==============================] - 0s 363us/step - loss: 1.0971 - acc: 0.4598 - val_loss: 1.2248 - val_acc: 0.3171\n",
      "Epoch 11/20\n",
      "361/361 [==============================] - 0s 371us/step - loss: 1.0928 - acc: 0.4460 - val_loss: 1.2215 - val_acc: 0.2927\n",
      "Epoch 12/20\n",
      "361/361 [==============================] - 0s 359us/step - loss: 1.0890 - acc: 0.4460 - val_loss: 1.2199 - val_acc: 0.2927\n",
      "Epoch 13/20\n",
      "361/361 [==============================] - 0s 373us/step - loss: 1.0857 - acc: 0.4488 - val_loss: 1.2171 - val_acc: 0.2927\n",
      "Epoch 14/20\n",
      "361/361 [==============================] - 0s 344us/step - loss: 1.0826 - acc: 0.4404 - val_loss: 1.2153 - val_acc: 0.2927\n",
      "Epoch 15/20\n",
      "361/361 [==============================] - 0s 366us/step - loss: 1.0802 - acc: 0.4515 - val_loss: 1.2141 - val_acc: 0.2927\n",
      "Epoch 16/20\n",
      "361/361 [==============================] - 0s 353us/step - loss: 1.0778 - acc: 0.4515 - val_loss: 1.2125 - val_acc: 0.2927\n",
      "Epoch 17/20\n",
      "361/361 [==============================] - 0s 338us/step - loss: 1.0757 - acc: 0.4571 - val_loss: 1.2098 - val_acc: 0.2927\n",
      "Epoch 18/20\n",
      "361/361 [==============================] - 0s 366us/step - loss: 1.0734 - acc: 0.4404 - val_loss: 1.2086 - val_acc: 0.2927\n",
      "Epoch 19/20\n",
      "361/361 [==============================] - 0s 374us/step - loss: 1.0717 - acc: 0.4460 - val_loss: 1.2069 - val_acc: 0.2927\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 0s 371us/step - loss: 1.0702 - acc: 0.4543 - val_loss: 1.2056 - val_acc: 0.2927\n",
      "Running Fold 4 / 10\n",
      "Train on 361 samples, validate on 41 samples\n",
      "Epoch 1/20\n",
      "361/361 [==============================] - 3s 9ms/step - loss: 1.2356 - acc: 0.3934 - val_loss: 1.2973 - val_acc: 0.3171\n",
      "Epoch 2/20\n",
      "361/361 [==============================] - 0s 401us/step - loss: 1.2040 - acc: 0.3989 - val_loss: 1.2800 - val_acc: 0.3171\n",
      "Epoch 3/20\n",
      "361/361 [==============================] - 0s 409us/step - loss: 1.1758 - acc: 0.3961 - val_loss: 1.2672 - val_acc: 0.3415\n",
      "Epoch 4/20\n",
      "361/361 [==============================] - 0s 380us/step - loss: 1.1548 - acc: 0.3934 - val_loss: 1.2578 - val_acc: 0.3415\n",
      "Epoch 5/20\n",
      "361/361 [==============================] - 0s 396us/step - loss: 1.1373 - acc: 0.3961 - val_loss: 1.2488 - val_acc: 0.3659\n",
      "Epoch 6/20\n",
      "361/361 [==============================] - 0s 388us/step - loss: 1.1230 - acc: 0.4017 - val_loss: 1.2422 - val_acc: 0.3902\n",
      "Epoch 7/20\n",
      "361/361 [==============================] - 0s 395us/step - loss: 1.1112 - acc: 0.4100 - val_loss: 1.2357 - val_acc: 0.3659\n",
      "Epoch 8/20\n",
      "361/361 [==============================] - 0s 394us/step - loss: 1.1015 - acc: 0.4183 - val_loss: 1.2293 - val_acc: 0.3902\n",
      "Epoch 9/20\n",
      "361/361 [==============================] - 0s 410us/step - loss: 1.0932 - acc: 0.4238 - val_loss: 1.2246 - val_acc: 0.4146\n",
      "Epoch 10/20\n",
      "361/361 [==============================] - 0s 409us/step - loss: 1.0864 - acc: 0.4127 - val_loss: 1.2205 - val_acc: 0.4146\n",
      "Epoch 11/20\n",
      "361/361 [==============================] - 0s 410us/step - loss: 1.0803 - acc: 0.4072 - val_loss: 1.2148 - val_acc: 0.4390\n",
      "Epoch 12/20\n",
      "361/361 [==============================] - 0s 398us/step - loss: 1.0758 - acc: 0.4044 - val_loss: 1.2122 - val_acc: 0.4390\n",
      "Epoch 13/20\n",
      "361/361 [==============================] - 0s 572us/step - loss: 1.0709 - acc: 0.4127 - val_loss: 1.2091 - val_acc: 0.4390\n",
      "Epoch 14/20\n",
      "361/361 [==============================] - 0s 525us/step - loss: 1.0672 - acc: 0.4211 - val_loss: 1.2058 - val_acc: 0.4878\n",
      "Epoch 15/20\n",
      "361/361 [==============================] - 0s 527us/step - loss: 1.0641 - acc: 0.4266 - val_loss: 1.2027 - val_acc: 0.4390\n",
      "Epoch 16/20\n",
      "361/361 [==============================] - ETA: 0s - loss: 1.0623 - acc: 0.429 - 0s 474us/step - loss: 1.0610 - acc: 0.4404 - val_loss: 1.1998 - val_acc: 0.4634\n",
      "Epoch 17/20\n",
      "361/361 [==============================] - 0s 517us/step - loss: 1.0587 - acc: 0.4488 - val_loss: 1.1969 - val_acc: 0.4634\n",
      "Epoch 18/20\n",
      "361/361 [==============================] - 0s 539us/step - loss: 1.0563 - acc: 0.4571 - val_loss: 1.1957 - val_acc: 0.4634\n",
      "Epoch 19/20\n",
      "361/361 [==============================] - 0s 542us/step - loss: 1.0542 - acc: 0.4626 - val_loss: 1.1953 - val_acc: 0.4634\n",
      "Epoch 20/20\n",
      "361/361 [==============================] - 0s 467us/step - loss: 1.0525 - acc: 0.4598 - val_loss: 1.1945 - val_acc: 0.4878\n",
      "Running Fold 5 / 10\n",
      "Train on 362 samples, validate on 40 samples\n",
      "Epoch 1/20\n",
      "362/362 [==============================] - 3s 8ms/step - loss: 1.4497 - acc: 0.3260 - val_loss: 1.6247 - val_acc: 0.2000\n",
      "Epoch 2/20\n",
      "362/362 [==============================] - 0s 430us/step - loss: 1.3647 - acc: 0.3536 - val_loss: 1.5493 - val_acc: 0.2500\n",
      "Epoch 3/20\n",
      "362/362 [==============================] - 0s 412us/step - loss: 1.3059 - acc: 0.3591 - val_loss: 1.4940 - val_acc: 0.1750\n",
      "Epoch 4/20\n",
      "362/362 [==============================] - 0s 384us/step - loss: 1.2650 - acc: 0.3674 - val_loss: 1.4487 - val_acc: 0.1750\n",
      "Epoch 5/20\n",
      "362/362 [==============================] - 0s 395us/step - loss: 1.2345 - acc: 0.3619 - val_loss: 1.4142 - val_acc: 0.1750\n",
      "Epoch 6/20\n",
      "362/362 [==============================] - 0s 395us/step - loss: 1.2119 - acc: 0.3564 - val_loss: 1.3842 - val_acc: 0.1750\n",
      "Epoch 7/20\n",
      "362/362 [==============================] - 0s 387us/step - loss: 1.1941 - acc: 0.3619 - val_loss: 1.3603 - val_acc: 0.2500\n",
      "Epoch 8/20\n",
      "362/362 [==============================] - 0s 409us/step - loss: 1.1804 - acc: 0.3757 - val_loss: 1.3393 - val_acc: 0.2500\n",
      "Epoch 9/20\n",
      "362/362 [==============================] - 0s 386us/step - loss: 1.1694 - acc: 0.3729 - val_loss: 1.3200 - val_acc: 0.2500\n",
      "Epoch 10/20\n",
      "362/362 [==============================] - 0s 404us/step - loss: 1.1602 - acc: 0.3812 - val_loss: 1.3036 - val_acc: 0.2500\n",
      "Epoch 11/20\n",
      "362/362 [==============================] - 0s 361us/step - loss: 1.1517 - acc: 0.3895 - val_loss: 1.2901 - val_acc: 0.2750\n",
      "Epoch 12/20\n",
      "362/362 [==============================] - 0s 383us/step - loss: 1.1449 - acc: 0.3950 - val_loss: 1.2784 - val_acc: 0.2750\n",
      "Epoch 13/20\n",
      "362/362 [==============================] - 0s 390us/step - loss: 1.1381 - acc: 0.3895 - val_loss: 1.2681 - val_acc: 0.2750\n",
      "Epoch 14/20\n",
      "362/362 [==============================] - 0s 391us/step - loss: 1.1322 - acc: 0.3950 - val_loss: 1.2571 - val_acc: 0.2750\n",
      "Epoch 15/20\n",
      "362/362 [==============================] - 0s 377us/step - loss: 1.1273 - acc: 0.3923 - val_loss: 1.2481 - val_acc: 0.2750\n",
      "Epoch 16/20\n",
      "362/362 [==============================] - 0s 364us/step - loss: 1.1220 - acc: 0.3978 - val_loss: 1.2402 - val_acc: 0.2750\n",
      "Epoch 17/20\n",
      "362/362 [==============================] - 0s 366us/step - loss: 1.1178 - acc: 0.4033 - val_loss: 1.2329 - val_acc: 0.3000\n",
      "Epoch 18/20\n",
      "362/362 [==============================] - 0s 399us/step - loss: 1.1136 - acc: 0.3950 - val_loss: 1.2248 - val_acc: 0.3000\n",
      "Epoch 19/20\n",
      "362/362 [==============================] - 0s 359us/step - loss: 1.1097 - acc: 0.4006 - val_loss: 1.2183 - val_acc: 0.3000\n",
      "Epoch 20/20\n",
      "362/362 [==============================] - 0s 379us/step - loss: 1.1061 - acc: 0.3895 - val_loss: 1.2114 - val_acc: 0.2750\n",
      "Running Fold 6 / 10\n",
      "Train on 362 samples, validate on 40 samples\n",
      "Epoch 1/20\n",
      "362/362 [==============================] - 3s 8ms/step - loss: 1.4917 - acc: 0.3785 - val_loss: 1.3634 - val_acc: 0.2750\n",
      "Epoch 2/20\n",
      "362/362 [==============================] - 0s 384us/step - loss: 1.4051 - acc: 0.3895 - val_loss: 1.2939 - val_acc: 0.3000\n",
      "Epoch 3/20\n",
      "362/362 [==============================] - 0s 369us/step - loss: 1.3359 - acc: 0.3757 - val_loss: 1.2414 - val_acc: 0.3250\n",
      "Epoch 4/20\n",
      "362/362 [==============================] - 0s 402us/step - loss: 1.2816 - acc: 0.3757 - val_loss: 1.2036 - val_acc: 0.3500\n",
      "Epoch 5/20\n",
      "362/362 [==============================] - 0s 393us/step - loss: 1.2412 - acc: 0.3702 - val_loss: 1.1757 - val_acc: 0.3750\n",
      "Epoch 6/20\n",
      "362/362 [==============================] - 0s 419us/step - loss: 1.2111 - acc: 0.3923 - val_loss: 1.1550 - val_acc: 0.4250\n",
      "Epoch 7/20\n",
      "362/362 [==============================] - 0s 409us/step - loss: 1.1870 - acc: 0.3978 - val_loss: 1.1390 - val_acc: 0.4750\n",
      "Epoch 8/20\n",
      "362/362 [==============================] - 0s 411us/step - loss: 1.1682 - acc: 0.3978 - val_loss: 1.1274 - val_acc: 0.4250\n",
      "Epoch 9/20\n",
      "362/362 [==============================] - 0s 388us/step - loss: 1.1530 - acc: 0.4088 - val_loss: 1.1190 - val_acc: 0.4250\n",
      "Epoch 10/20\n",
      "362/362 [==============================] - 0s 375us/step - loss: 1.1409 - acc: 0.4061 - val_loss: 1.1124 - val_acc: 0.4000\n",
      "Epoch 11/20\n",
      "362/362 [==============================] - 0s 415us/step - loss: 1.1312 - acc: 0.4088 - val_loss: 1.1077 - val_acc: 0.4250\n",
      "Epoch 12/20\n",
      "362/362 [==============================] - 0s 405us/step - loss: 1.1235 - acc: 0.4061 - val_loss: 1.1031 - val_acc: 0.4750\n",
      "Epoch 13/20\n",
      "362/362 [==============================] - 0s 430us/step - loss: 1.1160 - acc: 0.4006 - val_loss: 1.0995 - val_acc: 0.4750\n",
      "Epoch 14/20\n",
      "362/362 [==============================] - 0s 488us/step - loss: 1.1096 - acc: 0.3978 - val_loss: 1.0971 - val_acc: 0.4750\n",
      "Epoch 15/20\n",
      "362/362 [==============================] - 0s 520us/step - loss: 1.1044 - acc: 0.4006 - val_loss: 1.0946 - val_acc: 0.4750\n",
      "Epoch 16/20\n",
      "362/362 [==============================] - 0s 437us/step - loss: 1.0996 - acc: 0.4088 - val_loss: 1.0919 - val_acc: 0.4750\n",
      "Epoch 17/20\n",
      "362/362 [==============================] - 0s 411us/step - loss: 1.0952 - acc: 0.4116 - val_loss: 1.0895 - val_acc: 0.4750\n",
      "Epoch 18/20\n",
      "362/362 [==============================] - 0s 416us/step - loss: 1.0917 - acc: 0.4254 - val_loss: 1.0882 - val_acc: 0.4750\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362/362 [==============================] - 0s 412us/step - loss: 1.0884 - acc: 0.4199 - val_loss: 1.0866 - val_acc: 0.4750\n",
      "Epoch 20/20\n",
      "362/362 [==============================] - 0s 412us/step - loss: 1.0847 - acc: 0.4254 - val_loss: 1.0850 - val_acc: 0.4500\n",
      "Running Fold 7 / 10\n",
      "Train on 363 samples, validate on 39 samples\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 3s 8ms/step - loss: 1.8710 - acc: 0.3388 - val_loss: 1.5334 - val_acc: 0.3590\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 383us/step - loss: 1.7230 - acc: 0.3444 - val_loss: 1.4547 - val_acc: 0.3333\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 389us/step - loss: 1.6002 - acc: 0.3581 - val_loss: 1.3950 - val_acc: 0.3333\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 368us/step - loss: 1.5013 - acc: 0.3636 - val_loss: 1.3538 - val_acc: 0.3590\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 363us/step - loss: 1.4225 - acc: 0.3691 - val_loss: 1.3259 - val_acc: 0.3590\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 357us/step - loss: 1.3595 - acc: 0.3829 - val_loss: 1.3085 - val_acc: 0.3846\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 368us/step - loss: 1.3104 - acc: 0.3829 - val_loss: 1.2952 - val_acc: 0.3590\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 354us/step - loss: 1.2714 - acc: 0.4050 - val_loss: 1.2829 - val_acc: 0.3590\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 369us/step - loss: 1.2399 - acc: 0.3939 - val_loss: 1.2733 - val_acc: 0.3590\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 368us/step - loss: 1.2131 - acc: 0.3994 - val_loss: 1.2653 - val_acc: 0.3590\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 380us/step - loss: 1.1917 - acc: 0.3884 - val_loss: 1.2566 - val_acc: 0.3333\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 367us/step - loss: 1.1728 - acc: 0.3691 - val_loss: 1.2486 - val_acc: 0.3333\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 356us/step - loss: 1.1562 - acc: 0.3774 - val_loss: 1.2387 - val_acc: 0.2821\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 343us/step - loss: 1.1424 - acc: 0.3829 - val_loss: 1.2343 - val_acc: 0.2821\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 375us/step - loss: 1.1305 - acc: 0.3774 - val_loss: 1.2269 - val_acc: 0.2821\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 356us/step - loss: 1.1191 - acc: 0.3802 - val_loss: 1.2218 - val_acc: 0.2564\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 387us/step - loss: 1.1103 - acc: 0.3774 - val_loss: 1.2188 - val_acc: 0.2564\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 354us/step - loss: 1.1020 - acc: 0.3857 - val_loss: 1.2179 - val_acc: 0.2308\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 363us/step - loss: 1.0945 - acc: 0.4050 - val_loss: 1.2165 - val_acc: 0.2564\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 351us/step - loss: 1.0886 - acc: 0.4050 - val_loss: 1.2161 - val_acc: 0.2308\n",
      "Running Fold 8 / 10\n",
      "Train on 363 samples, validate on 39 samples\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 3s 9ms/step - loss: 1.5354 - acc: 0.3526 - val_loss: 1.4771 - val_acc: 0.4103\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 385us/step - loss: 1.4758 - acc: 0.3499 - val_loss: 1.4326 - val_acc: 0.4103\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 369us/step - loss: 1.4249 - acc: 0.3554 - val_loss: 1.3932 - val_acc: 0.4103\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 361us/step - loss: 1.3799 - acc: 0.3526 - val_loss: 1.3576 - val_acc: 0.4103\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 372us/step - loss: 1.3413 - acc: 0.3691 - val_loss: 1.3249 - val_acc: 0.4103\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 358us/step - loss: 1.3070 - acc: 0.3636 - val_loss: 1.2963 - val_acc: 0.4103\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 397us/step - loss: 1.2781 - acc: 0.3636 - val_loss: 1.2704 - val_acc: 0.4103\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 368us/step - loss: 1.2521 - acc: 0.3636 - val_loss: 1.2471 - val_acc: 0.4103\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 340us/step - loss: 1.2298 - acc: 0.3747 - val_loss: 1.2264 - val_acc: 0.4103\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 360us/step - loss: 1.2102 - acc: 0.3774 - val_loss: 1.2065 - val_acc: 0.4103\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 375us/step - loss: 1.1930 - acc: 0.3691 - val_loss: 1.1892 - val_acc: 0.4103\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 383us/step - loss: 1.1774 - acc: 0.3774 - val_loss: 1.1737 - val_acc: 0.4103\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 380us/step - loss: 1.1641 - acc: 0.3829 - val_loss: 1.1602 - val_acc: 0.4359\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 385us/step - loss: 1.1524 - acc: 0.3967 - val_loss: 1.1490 - val_acc: 0.4615\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 459us/step - loss: 1.1411 - acc: 0.3884 - val_loss: 1.1386 - val_acc: 0.4615\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 382us/step - loss: 1.1317 - acc: 0.3994 - val_loss: 1.1293 - val_acc: 0.4615\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 390us/step - loss: 1.1227 - acc: 0.4050 - val_loss: 1.1212 - val_acc: 0.4615\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 342us/step - loss: 1.1157 - acc: 0.4105 - val_loss: 1.1139 - val_acc: 0.4359\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 351us/step - loss: 1.1086 - acc: 0.4215 - val_loss: 1.1071 - val_acc: 0.4359\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 334us/step - loss: 1.1022 - acc: 0.4353 - val_loss: 1.1018 - val_acc: 0.4359\n",
      "Running Fold 9 / 10\n",
      "Train on 363 samples, validate on 39 samples\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 3s 8ms/step - loss: 1.7568 - acc: 0.3168 - val_loss: 1.5601 - val_acc: 0.2821\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 434us/step - loss: 1.6206 - acc: 0.3223 - val_loss: 1.4247 - val_acc: 0.3590\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 368us/step - loss: 1.5092 - acc: 0.3168 - val_loss: 1.3199 - val_acc: 0.3846\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 339us/step - loss: 1.4222 - acc: 0.3416 - val_loss: 1.2440 - val_acc: 0.4359\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 369us/step - loss: 1.3547 - acc: 0.3499 - val_loss: 1.1917 - val_acc: 0.4359\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 402us/step - loss: 1.3033 - acc: 0.3526 - val_loss: 1.1543 - val_acc: 0.4615\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 409us/step - loss: 1.2639 - acc: 0.3636 - val_loss: 1.1262 - val_acc: 0.4103\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 365us/step - loss: 1.2315 - acc: 0.3802 - val_loss: 1.1046 - val_acc: 0.4103\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 448us/step - loss: 1.2062 - acc: 0.3802 - val_loss: 1.0874 - val_acc: 0.4359\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 380us/step - loss: 1.1851 - acc: 0.3912 - val_loss: 1.0734 - val_acc: 0.4359\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 411us/step - loss: 1.1681 - acc: 0.4050 - val_loss: 1.0623 - val_acc: 0.4103\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 313us/step - loss: 1.1533 - acc: 0.3994 - val_loss: 1.0534 - val_acc: 0.4359\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 349us/step - loss: 1.1411 - acc: 0.3912 - val_loss: 1.0465 - val_acc: 0.4359\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 543us/step - loss: 1.1307 - acc: 0.3884 - val_loss: 1.0409 - val_acc: 0.4872\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 414us/step - loss: 1.1220 - acc: 0.3994 - val_loss: 1.0365 - val_acc: 0.4872\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 407us/step - loss: 1.1149 - acc: 0.3994 - val_loss: 1.0332 - val_acc: 0.4615\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 409us/step - loss: 1.1082 - acc: 0.4160 - val_loss: 1.0301 - val_acc: 0.4615\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 365us/step - loss: 1.1022 - acc: 0.4187 - val_loss: 1.0276 - val_acc: 0.4872\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 376us/step - loss: 1.0981 - acc: 0.4298 - val_loss: 1.0258 - val_acc: 0.4615\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 351us/step - loss: 1.0930 - acc: 0.4380 - val_loss: 1.0243 - val_acc: 0.4615\n",
      "Running Fold 10 / 10\n",
      "Train on 363 samples, validate on 39 samples\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 3s 9ms/step - loss: 1.2594 - acc: 0.3774 - val_loss: 1.3265 - val_acc: 0.3333\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 387us/step - loss: 1.2319 - acc: 0.3802 - val_loss: 1.3143 - val_acc: 0.3333\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 367us/step - loss: 1.2078 - acc: 0.3884 - val_loss: 1.3062 - val_acc: 0.3333\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 353us/step - loss: 1.1876 - acc: 0.3994 - val_loss: 1.2970 - val_acc: 0.3846\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 385us/step - loss: 1.1709 - acc: 0.4050 - val_loss: 1.2874 - val_acc: 0.3590\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 385us/step - loss: 1.1566 - acc: 0.4105 - val_loss: 1.2796 - val_acc: 0.3077\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 347us/step - loss: 1.1434 - acc: 0.4077 - val_loss: 1.2730 - val_acc: 0.3333\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 333us/step - loss: 1.1327 - acc: 0.4160 - val_loss: 1.2641 - val_acc: 0.3333\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 335us/step - loss: 1.1232 - acc: 0.4298 - val_loss: 1.2559 - val_acc: 0.3077\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 327us/step - loss: 1.1151 - acc: 0.4325 - val_loss: 1.2491 - val_acc: 0.3077\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 331us/step - loss: 1.1082 - acc: 0.4325 - val_loss: 1.2418 - val_acc: 0.3077\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 356us/step - loss: 1.1015 - acc: 0.4298 - val_loss: 1.2336 - val_acc: 0.3077\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 360us/step - loss: 1.0960 - acc: 0.4270 - val_loss: 1.2263 - val_acc: 0.3077\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - ETA: 0s - loss: 1.0969 - acc: 0.426 - 0s 343us/step - loss: 1.0916 - acc: 0.4298 - val_loss: 1.2206 - val_acc: 0.2821\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 394us/step - loss: 1.0874 - acc: 0.4298 - val_loss: 1.2163 - val_acc: 0.3077\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 397us/step - loss: 1.0832 - acc: 0.4325 - val_loss: 1.2116 - val_acc: 0.2821\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 409us/step - loss: 1.0799 - acc: 0.4380 - val_loss: 1.2059 - val_acc: 0.3077\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 376us/step - loss: 1.0764 - acc: 0.4325 - val_loss: 1.2012 - val_acc: 0.2821\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 383us/step - loss: 1.0737 - acc: 0.4325 - val_loss: 1.1970 - val_acc: 0.2821\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 396us/step - loss: 1.0712 - acc: 0.4353 - val_loss: 1.1907 - val_acc: 0.2821\n"
     ]
    }
   ],
   "source": [
    "n_folds = 10\n",
    "\n",
    "labels = df['flow_rate'].values\n",
    "data = x\n",
    "\n",
    "labels[labels == 1800] = 0\n",
    "labels[labels == 3600] = 1\n",
    "labels[labels == 7200] = 2\n",
    "\n",
    "labels = np_utils.to_categorical(labels, nb_classes) \n",
    "\n",
    "skf = StratifiedKFold(df['flow_rate'].values, n_folds=n_folds, shuffle=True)\n",
    "avg_acc = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "    model = None # Clearing the NN.\n",
    "    model = build_logistic_model(input_dim, nb_classes)\n",
    "    \n",
    "    std = np.std(data[train],0)\n",
    "    mean = np.mean(data[train],0)\n",
    "    \n",
    "    x_train = (data[train]-mean)/std\n",
    "    x_test = (data[test] - mean)/std        \n",
    "    \n",
    "    avg_acc += train_and_evaluate_model(model, x_train, labels[train], x_test, labels[test])\n",
    "    \n",
    "    # Predict the values from the validation dataset\n",
    "    Y_pred = model.predict(x_test)\n",
    "    # Convert predictions classes to one hot vectors \n",
    "    Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "    # Convert validation observations to one hot vectors\n",
    "    Y_true = np.argmax(labels[test],axis = 1) \n",
    "    \n",
    "    y_true.extend(Y_true)\n",
    "    y_pred.extend(Y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score:  0.3653839464690798\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Score: \", avg_acc/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(np.array(y_true), np.array(y_pred)) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(3)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
