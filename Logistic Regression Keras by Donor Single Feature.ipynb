{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet_ml import Experiment\n",
    "# experiment = Experiment(api_key=\"xktj4EX0zB8YcQ3BEaFwOQYpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\train\\\\\"\n",
    "train_csv = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\train\\\\ids.csv\"\n",
    "test_path = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\test\\\\\"\n",
    "test_csv = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\test\\\\ids.csv\"\n",
    "\n",
    "dest_path = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\\"\n",
    "\n",
    "csv = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\ids.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv,index_col=0).sample(frac=1)\n",
    "df = df[df['name'].str.contains(\"_flipped\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>flow_rate</th>\n",
       "      <th>source</th>\n",
       "      <th>peak_val</th>\n",
       "      <th>area</th>\n",
       "      <th>volume</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>euler_number</th>\n",
       "      <th>extent</th>\n",
       "      <th>mean_intensity</th>\n",
       "      <th>moments</th>\n",
       "      <th>orientation</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>inertia_tensor</th>\n",
       "      <th>cluster</th>\n",
       "      <th>num_peaks</th>\n",
       "      <th>sd</th>\n",
       "      <th>num_local_peaks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>159_flipped</td>\n",
       "      <td>2017.11.23</td>\n",
       "      <td>3600</td>\n",
       "      <td>12.20 s.tifheightmap.mat</td>\n",
       "      <td>9.024321</td>\n",
       "      <td>3211</td>\n",
       "      <td>16442.266280</td>\n",
       "      <td>0.920166</td>\n",
       "      <td>1</td>\n",
       "      <td>0.455074</td>\n",
       "      <td>5.120606</td>\n",
       "      <td>[[3.21100000e+03 1.52583000e+05 8.94101100e+06...</td>\n",
       "      <td>-0.954634</td>\n",
       "      <td>271.563492</td>\n",
       "      <td>[[ 320.08762545 -293.11866355]\\r\\n [-293.11866...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.758256</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>119_flipped</td>\n",
       "      <td>2017.11.22</td>\n",
       "      <td>7200</td>\n",
       "      <td>35.20 s.tifheightmap.mat</td>\n",
       "      <td>6.317282</td>\n",
       "      <td>1634</td>\n",
       "      <td>6125.725518</td>\n",
       "      <td>0.727004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.672428</td>\n",
       "      <td>3.748914</td>\n",
       "      <td>[[1.63400000e+03 3.82320000e+04 1.11278800e+06...</td>\n",
       "      <td>0.629859</td>\n",
       "      <td>168.811183</td>\n",
       "      <td>[[166.56046317  51.3137445 ]\\r\\n [ 51.3137445 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.005588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>120_flipped</td>\n",
       "      <td>2017.11.22</td>\n",
       "      <td>7200</td>\n",
       "      <td>35.80 s.tifheightmap.mat</td>\n",
       "      <td>4.854192</td>\n",
       "      <td>2055</td>\n",
       "      <td>5440.452402</td>\n",
       "      <td>0.946396</td>\n",
       "      <td>1</td>\n",
       "      <td>0.368544</td>\n",
       "      <td>2.647422</td>\n",
       "      <td>[[2.05500000e+03 9.37710000e+04 5.11496900e+06...</td>\n",
       "      <td>-0.886416</td>\n",
       "      <td>236.835570</td>\n",
       "      <td>[[ 292.97894613 -278.03612482]\\r\\n [-278.03612...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.399912</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>150_flipped</td>\n",
       "      <td>2017.11.23</td>\n",
       "      <td>1800</td>\n",
       "      <td>74.00 s.tifheightmap.mat</td>\n",
       "      <td>17.317134</td>\n",
       "      <td>2318</td>\n",
       "      <td>21528.303651</td>\n",
       "      <td>0.782971</td>\n",
       "      <td>1</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>9.287448</td>\n",
       "      <td>[[2.31800000e+03 8.37060000e+04 3.76076000e+06...</td>\n",
       "      <td>-1.569404</td>\n",
       "      <td>200.509668</td>\n",
       "      <td>[[ 1.23203633e+02 -2.71778206e-01]\\r\\n [-2.717...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.217220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>46_flipped</td>\n",
       "      <td>2017.11.17</td>\n",
       "      <td>3600</td>\n",
       "      <td>25.60 s.tifheightmap.mat</td>\n",
       "      <td>4.974612</td>\n",
       "      <td>470</td>\n",
       "      <td>1243.052519</td>\n",
       "      <td>0.888538</td>\n",
       "      <td>1</td>\n",
       "      <td>0.725309</td>\n",
       "      <td>2.644793</td>\n",
       "      <td>[[4.70000000e+02 7.36900000e+03 1.54739000e+05...</td>\n",
       "      <td>1.456527</td>\n",
       "      <td>89.941125</td>\n",
       "      <td>[[18.60479855  7.53689452]\\r\\n [ 7.53689452 83...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.564867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name        date  flow_rate                    source   peak_val  \\\n",
       "268  159_flipped  2017.11.23       3600  12.20 s.tifheightmap.mat   9.024321   \n",
       "224  119_flipped  2017.11.22       7200  35.20 s.tifheightmap.mat   6.317282   \n",
       "226  120_flipped  2017.11.22       7200  35.80 s.tifheightmap.mat   4.854192   \n",
       "259  150_flipped  2017.11.23       1800  74.00 s.tifheightmap.mat  17.317134   \n",
       "343   46_flipped  2017.11.17       3600  25.60 s.tifheightmap.mat   4.974612   \n",
       "\n",
       "     area        volume  eccentricity  euler_number    extent  mean_intensity  \\\n",
       "268  3211  16442.266280      0.920166             1  0.455074        5.120606   \n",
       "224  1634   6125.725518      0.727004             1  0.672428        3.748914   \n",
       "226  2055   5440.452402      0.946396             1  0.368544        2.647422   \n",
       "259  2318  21528.303651      0.782971             1  0.677778        9.287448   \n",
       "343   470   1243.052519      0.888538             1  0.725309        2.644793   \n",
       "\n",
       "                                               moments  orientation  \\\n",
       "268  [[3.21100000e+03 1.52583000e+05 8.94101100e+06...    -0.954634   \n",
       "224  [[1.63400000e+03 3.82320000e+04 1.11278800e+06...     0.629859   \n",
       "226  [[2.05500000e+03 9.37710000e+04 5.11496900e+06...    -0.886416   \n",
       "259  [[2.31800000e+03 8.37060000e+04 3.76076000e+06...    -1.569404   \n",
       "343  [[4.70000000e+02 7.36900000e+03 1.54739000e+05...     1.456527   \n",
       "\n",
       "      perimeter                                     inertia_tensor  cluster  \\\n",
       "268  271.563492  [[ 320.08762545 -293.11866355]\\r\\n [-293.11866...        0   \n",
       "224  168.811183  [[166.56046317  51.3137445 ]\\r\\n [ 51.3137445 ...        2   \n",
       "226  236.835570  [[ 292.97894613 -278.03612482]\\r\\n [-278.03612...        2   \n",
       "259  200.509668  [[ 1.23203633e+02 -2.71778206e-01]\\r\\n [-2.717...        0   \n",
       "343   89.941125  [[18.60479855  7.53689452]\\r\\n [ 7.53689452 83...        2   \n",
       "\n",
       "     num_peaks        sd  num_local_peaks  \n",
       "268          1  2.758256                5  \n",
       "224          1  2.005588                1  \n",
       "226          1  1.399912                3  \n",
       "259          1  5.217220                1  \n",
       "343          1  1.564867                1  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample to balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "2017.11.09    22\n",
      "2017.11.17    22\n",
      "2017.11.22    22\n",
      "2017.11.23    22\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ds_count = min(df.groupby('date').size())\n",
    "ds_flow = df.groupby('date').size().idxmin()\n",
    "\n",
    "samples = pd.concat([df[df['date']==i].sample(n=ds_count) \n",
    "                     for i in df.date.unique()],ignore_index=True)\n",
    "\n",
    "df = samples.sample(frac=1)\n",
    "\n",
    "print(samples.groupby('date').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns which we aren't using as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(df):\n",
    "    return df.drop(columns=['name','date','flow_rate','source','moments',\n",
    "                            'inertia_tensor','euler_number','num_peaks','cluster']).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_volume'] = df['volume'].apply(np.log)\n",
    "\n",
    "x = df.drop(columns=['name','date','flow_rate','source','moments','inertia_tensor'])\n",
    "\n",
    "# Drop catagorical  features\n",
    "mat = x.drop(columns=['euler_number', 'num_peaks', 'cluster'])\n",
    "\n",
    "x = mat.as_matrix()\n",
    "# x = standardize = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into balanced test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 11)\n",
      "(8, 11)\n"
     ]
    }
   ],
   "source": [
    "x_test = pd.concat([df[df['date']==i].sample(n=2) for i in df.date.unique()],ignore_index=True)\n",
    "y_test = x_test['date'].values\n",
    "\n",
    "x_train = pd.concat([df, x_test, x_test]).drop_duplicates(keep=False)\n",
    "y_train = x_train['date'].values\n",
    "\n",
    "x_test = drop_cols(x_test)\n",
    "x_train = drop_cols(x_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "std = np.std(x_train,0)\n",
    "mean = np.mean(x_train,0)\n",
    "    \n",
    "x_train = (x_train-mean)/std\n",
    "x_test = (x_test-mean)/std    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "nb_classes = 4\n",
    "nb_epoch = 30\n",
    "\n",
    "lmda = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[1]\n",
    "#input_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train == '2017.11.09'] = 0\n",
    "y_train[y_train == '2017.11.17'] = 1\n",
    "y_train[y_train == '2017.11.22'] = 2\n",
    "y_train[y_train == '2017.11.23'] = 3\n",
    "\n",
    "y_test[y_test == '2017.11.09'] = 0\n",
    "y_test[y_test == '2017.11.17'] = 1\n",
    "y_test[y_test == '2017.11.22'] = 2\n",
    "y_test[y_test == '2017.11.23'] = 3\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes) \n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logistic_model(input_dim, output_dim, lmda):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim, input_dim=input_dim,\n",
    "                    kernel_regularizer=l2(lmda),\n",
    "                    activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_model(input_dim, output_dim, lmda, optimizer):\n",
    "    model = build_logistic_model(input_dim,output_dim,lmda)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model = build_logistic_model(input_dim, nb_classes,lmda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 4)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 4)                 48        \n",
      "=================================================================\n",
      "Total params: 48\n",
      "Trainable params: 48\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 8 samples\n",
      "Epoch 1/30\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 1.5946 - acc: 0.2500 - val_loss: 1.4393 - val_acc: 0.3750\n",
      "Epoch 2/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4314 - acc: 0.3000 - val_loss: 1.2843 - val_acc: 0.3750\n",
      "Epoch 3/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3475 - acc: 0.3750 - val_loss: 1.2647 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2996 - acc: 0.4500 - val_loss: 1.2454 - val_acc: 0.5000\n",
      "Epoch 5/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2626 - acc: 0.4500 - val_loss: 1.2219 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2353 - acc: 0.4625 - val_loss: 1.2061 - val_acc: 0.5000\n",
      "Epoch 7/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2123 - acc: 0.4625 - val_loss: 1.1967 - val_acc: 0.5000\n",
      "Epoch 8/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1961 - acc: 0.4625 - val_loss: 1.1919 - val_acc: 0.5000\n",
      "Epoch 9/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1824 - acc: 0.4500 - val_loss: 1.1955 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1682 - acc: 0.4750 - val_loss: 1.1983 - val_acc: 0.5000\n",
      "Epoch 11/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1597 - acc: 0.4750 - val_loss: 1.2022 - val_acc: 0.3750\n",
      "Epoch 12/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1511 - acc: 0.5125 - val_loss: 1.2016 - val_acc: 0.3750\n",
      "Epoch 13/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1435 - acc: 0.5125 - val_loss: 1.1883 - val_acc: 0.3750\n",
      "Epoch 14/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1376 - acc: 0.5250 - val_loss: 1.2023 - val_acc: 0.3750\n",
      "Epoch 15/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1297 - acc: 0.5375 - val_loss: 1.2011 - val_acc: 0.3750\n",
      "Epoch 16/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1258 - acc: 0.5125 - val_loss: 1.1974 - val_acc: 0.3750\n",
      "Epoch 17/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1214 - acc: 0.5500 - val_loss: 1.1970 - val_acc: 0.3750\n",
      "Epoch 18/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1173 - acc: 0.5125 - val_loss: 1.1993 - val_acc: 0.3750\n",
      "Epoch 19/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1128 - acc: 0.5250 - val_loss: 1.1927 - val_acc: 0.3750\n",
      "Epoch 20/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1090 - acc: 0.5625 - val_loss: 1.1967 - val_acc: 0.3750\n",
      "Epoch 21/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1063 - acc: 0.5500 - val_loss: 1.1960 - val_acc: 0.3750\n",
      "Epoch 22/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1031 - acc: 0.5500 - val_loss: 1.1969 - val_acc: 0.3750\n",
      "Epoch 23/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1002 - acc: 0.5625 - val_loss: 1.2026 - val_acc: 0.3750\n",
      "Epoch 24/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0964 - acc: 0.5375 - val_loss: 1.1991 - val_acc: 0.3750\n",
      "Epoch 25/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0941 - acc: 0.6125 - val_loss: 1.1963 - val_acc: 0.3750\n",
      "Epoch 26/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0932 - acc: 0.5375 - val_loss: 1.1960 - val_acc: 0.3750\n",
      "Epoch 27/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0907 - acc: 0.5250 - val_loss: 1.1915 - val_acc: 0.3750\n",
      "Epoch 28/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0889 - acc: 0.5500 - val_loss: 1.2013 - val_acc: 0.3750\n",
      "Epoch 29/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0850 - acc: 0.5750 - val_loss: 1.1987 - val_acc: 0.3750\n",
      "Epoch 30/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0847 - acc: 0.5625 - val_loss: 1.2014 - val_acc: 0.3750\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size, epochs=nb_epoch,\n",
    "                    verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 1.2013959884643555\n",
      "Test accuracy: 0.375\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.20833623, 0.22617595, 0.41421658, 0.1512712 ],\n",
       "        [0.05304648, 0.5497235 , 0.26334792, 0.13388202],\n",
       "        [0.30580944, 0.07912634, 0.5757128 , 0.03935138],\n",
       "        [0.19598007, 0.23991157, 0.30125126, 0.2628571 ],\n",
       "        [0.06191311, 0.5757963 , 0.15632284, 0.20596772],\n",
       "        [0.23606144, 0.11572988, 0.5506102 , 0.09759849],\n",
       "        [0.12604155, 0.19349779, 0.05167484, 0.62878585],\n",
       "        [0.33705646, 0.0903523 , 0.12573935, 0.4468519 ]], dtype=float32), 1)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(x_test),1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEmCAYAAAAA6gkZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmYXVWZ7/HvLwlhkEEkiKYKQkiYEmRKggKtoAwmTKHVMIg2KC1qE2kE7SvIBQRtaLhXGpt4MbQIohCISpMgEJSWVvIQyABGkxAohkhVUBJAZA4J7/1j7wqHypkqObv2rnN+H579cIZ91np3nfO8WXuttddWRGBmZusakHcAZmZF5QRpZlaBE6SZWQVOkGZmFThBmplV4ARpZlaBE2QLkbSppJmSXpQ0fQPKOUnS3Y2MLS+SPixpad5xWDHJ8yCLR9KngbOA3YCXgIeB70TEfRtY7meBrwAHRMTqDQ604CQFsHNEdOQdi/VPbkEWjKSzgH8H/hXYDtgB+D4wsQHFDwMebYXkWA9Jg/KOwQouIrwVZAO2Al4GJlXZZ2OSBLo83f4d2Dh972CgEzgbeBZ4Bvhc+t63gFXAm2kdpwIXAj8pKXtHIIBB6fNTgCdIWrFPAieVvH5fyecOAOYCL6b/P6DkvXuBi4HZaTl3A0MqHFt3/P9SEv+xwBHAo8DzwLkl++8H3A/8Nd33KmBw+t5v02N5JT3e40vK/1/An4Ebul9LPzMirWPf9PlQYCVwcN6/DW/5bG5BFsv+wCbArVX2+SbwIWBvYC+SJHFeyfvvI0m0bSRJcIqkrSPiApJW6c0RsXlE/LBaIJLeBXwPmBARW5AkwYfL7Pce4JfpvtsA3wV+KWmbkt0+DXwOeC8wGPhalarfR/I3aAPOB64BPgOMAT4MnC9pp3TfNcBXgSEkf7tDgH8CiIiPpPvslR7vzSXlv4ekNX1aacUR8ThJ8vyppM2AHwHXRcS9VeK1JuYEWSzbACuj+inwScBFEfFsRKwgaRl+tuT9N9P334yIO0haT7uuZzxvAXtI2jQinomIRWX2ORJ4LCJuiIjVEXET8AhwdMk+P4qIRyPiNeAWkuReyZsk/a1vAtNIkt+VEfFSWv8iYE+AiJgfEXPSep8CfgAcVMcxXRARb6TxvENEXAM8BjwAvJ/kHyRrUU6QxfIcMKRG39hQYFnJ82Xpa2vL6JFgXwU2720gEfEKyWnpl4BnJP1S0m51xNMdU1vJ8z/3Ip7nImJN+rg7gf2l5P3Xuj8vaRdJt0v6s6S/kbSQh1QpG2BFRLxeY59rgD2A/4iIN2rsa03MCbJY7gdeJ+l3q2Q5yelhtx3S19bHK8BmJc/fV/pmRMyKiMNIWlKPkCSOWvF0x9S1njH1xv8jiWvniNgSOBdQjc9UnbYhaXOSft0fAhemXQjWopwgCyQiXiTpd5si6VhJm0naSNIESZelu90EnCdpW0lD0v1/sp5VPgx8RNIOkrYCzul+Q9J2ko5J+yLfIDlVX1OmjDuAXSR9WtIgSccDo4Db1zOm3tgC+Bvwctq6/XKP9/8C7LTOp6q7EpgfEf9I0rd69QZHaf2WE2TBRMR3SeZAngesAJ4GJgP/le7ybWAesBD4A7AgfW196voVcHNa1nzemdQGkIyGLycZ2T2IdACkRxnPAUel+z5HMgJ9VESsXJ+YeulrJANAL5G0bm/u8f6FwPWS/irpuFqFSZoIjCfpVoDke9hX0kkNi9j6FU8UNzOrwC1IM7MKnCDNrClIulbSs5L+WOF9SfqepA5JCyXtW6tMJ0gzaxbXkfQhVzIB2DndTiOZBVGVE6SZNYWI+C3JgGIlE4EfR2IO8G5J769WZqEu1h8yZEgMG7Zj3mFk7qElf8o7BGugfXbfIe8QMrds2VOsXLmy1hzTXhm45bCI1etczFRRvLZiEck84W5TI2JqL6psI5kV0q0zfe2ZSh8oVIIcNmxHZj8wL+8wMrf1uMl5h2ANNPuBq/IOIXMHfnBsw8uM1a+x8a41Z1+t9frDU16PiA0JpFyCrzqNp1AJ0sxaiUB92svXCWxf8rydGlehuQ/SzPIhQKp/23AzgH9IR7M/BLwYERVPr8EtSDPLUwNbkJJuIlnfc4ikTuACYCOAiLia5LLYI4AOkkVTPlerTCdIM8uJYMDAhpUWESfWeD+A03tTphOkmeWnMafOmXGCNLN8iL4epOk1J0gzy0nDBl8y4wRpZvlxC9LMrAK3IM3MyunzieK95gRpZvnoniheYE6QZpYftyDNzMoRDGzcRPEsOEGaWT48D9LMrAr3QZqZleNRbDOzytyCNDOrwC1IM7MyGrcQbmacIM0sPwVvQRY7ugzdPesu9hy9K6N3G8nll12adziZuPqCk1h2zyXMm35u3qFkqlWOE5rwd9u3t1zotZZMkGvWrOHMM07ntpl38tDCxUyfdhNLFi/OO6yGu2HmHCaePiXvMDLXKsfZfL/bdBS73i0HLZkg5z74ICNGjGT4TjsxePBgJh1/ArfPvC3vsBpu9oLHef7FV/MOI3OtcpxN97sVyS0X6t1y0JIJcvnyLtrb3777Y1tbO11dXTlGZFZb8/1uW7wFKWm8pKWSOiR9I8u6eiO5d887qeCjaWZN+bsteB9kZqPYkgYCU4DDSG7YPVfSjIjIvdOkra2dzs6n1z7v6upk6NChOUZkVltT/m5beBR7P6AjIp6IiFXANGBihvXVbey4cXR0PMZTTz7JqlWrmH7zNI486pi8wzKrqil/twVvQWaZINuAp0ued6avvYOk0yTNkzRvxcoVGYbztkGDBnHFlVdx9JEfZ+8P7M4nJx3HqNGj+6TuvnT9Jadw7/Vns8uw7ei462JOPnb/vEPKRKscZ9P9blX8PsgsJ4qXS/nrdKJExFRgKsCYMWPX7WTJyPgJRzB+whF9VV0uTj7nurxD6BOtcpzQhL/bgvehZpkgO4HtS563A8szrM/M+pmiDzJl2W6dC+wsabikwcAJwIwM6zOzfiS5JY3q3vKQWQsyIlZLmgzMAgYC10bEoqzqM7N+RkIDit2CzHSxioi4A7gjyzrMrP8q+im2V/Mxs9w4QZqZVeAEaWZWjig/GbBAnCDNLBciv9HpejlBmllunCDNzCpwgjQzq8AJ0sysHA/SmJmVJ8SAAa27HqSZWVWNvBa71h0MJO0g6TeSHpK0UFLNZZGcIM0sP+rFVq2Yt+9gMAEYBZwoaVSP3c4DbomIfUgWz/l+rfCcIM0sH2poC7KeOxgEsGX6eCvqWH7RfZBmlptejmIPkTSv5PnUdMFtKH8Hgw/2+PyFwN2SvgK8Czi0VoVOkGaWm14myJURMbZSUWVe63mHghOB6yLi/0raH7hB0h4R8ValCp0gzSwXDb7UsJ47GJwKjAeIiPslbQIMAZ6tVKj7IM0sPw0apKG+Oxj8CTgEQNLuwCZA1TsFugVpZvlQ466kqXQHA0kXAfMiYgZwNnCNpK+SnH6fEhFVbxToBGlmuWnkpYbl7mAQEeeXPF4MHNibMp0gzSw3LX1PGjOzarxYhZlZGXnezrVeTpBmlhsnSDOzCpwgzcwqKXZ+dII0s/y4BWlmVk4DJ4pnxQnSzHIhoOD50QnSzPIiBniiuJlZeT7FNjMrRz7FNjMrS+BTbDOzStyCNDOrwH2QZmbluA/SzKy8ZB5ksTOkE6SZ5cTLnZmZVVTw/OgEaWY5kaf5mJmV5T5IM7MqCp4fnSDNLD9uQZqZVVDw/OgEaWY58YK5ZmblecFcM7OKij9RfEDeAeTl7ll3sefoXRm920guv+zSvMPJxNUXnMSyey5h3vRz8w4lU61ynNB8v1up/i0PLZkg16xZw5lnnM5tM+/koYWLmT7tJpYsXpx3WA13w8w5TDx9St5hZK5VjrPpfrfpRPF6tzy0ZIKc++CDjBgxkuE77cTgwYOZdPwJ3D7ztrzDarjZCx7n+RdfzTuMzLXKcTbb77Z7oni9Wx5aMkEuX95Fe/v2a5+3tbXT1dWVY0RmtTXj77ZlE6SkayU9K+mPWdWxviJindeK3lls1oy/21bug7wOGJ9h+eutra2dzs6n1z7v6upk6NChOUZkVlsz/m5btgUZEb8Fns+q/A0xdtw4Ojoe46knn2TVqlVMv3kaRx51TN5hmVXVdL/bXrQem7EFWRdJp0maJ2neipUr+qTOQYMGccWVV3H0kR9n7w/szicnHceo0aP7pO6+dP0lp3Dv9Wezy7Dt6LjrYk4+dv+8Q8pEqxxns/1uRf2tx7xakCrXr9GwwqUdgdsjYo969h8zZmzMfmBeZvEUxdbjJucdgjXQC3OvyjuEzB34wbHMnz+voVlqyx12j3Ffv7bu/f/7jAPmR8TYRsZQi6+kMbPcDCj4IFPup9hm1roa2QcpabykpZI6JH2jwj7HSVosaZGkG2uVmVkLUtJNwMHAEEmdwAUR8cOs6jOz/kWCgQ26QkbSQGAKcBjQCcyVNCMiFpfsszNwDnBgRLwg6b21ys0sQUbEiVmVbWbNoYGDL/sBHRHxRFruNGAiUHot5heAKRHxAkBEPFur0IoJUtKW1T4YEX+rI2gzs4p6mR+HSCodxZ0aEVPTx23A0yXvdQIf7PH5XZI6NRsYCFwYEXdVq7BaC3IRECSXTHbrfh7ADtUKNjOrRiRTfXphZZVR7HIF9ZyiMwjYmaTrrx34naQ9IuKvlSqsmCAjYvtK75mZNUIDF+npBEpzVjuwvMw+cyLiTeBJSUtJEubcivHVU7OkEySdmz5ulzSmN5Gbma2jF5PE6+irnAvsLGm4pMHACcCMHvv8F/DRpGoNITnlfqJaoTUTpKSr0kI/m770KnB1rc+ZmdXSqGk+EbEamAzMApYAt0TEIkkXSeq+HnMW8JykxcBvgK9HxHPVyq1nFPuAiNhX0kNpIM+nGdrMbL2Jxk4Uj4g7gDt6vHZ+yeMAzkq3utSTIN+UNIC0w1PSNsBb9VZgZlZJwS+kqasPcgrwc2BbSd8C7gP+LdOozKwlFH2xipotyIj4saT5wKHpS5MionCL4JpZ/9LIK2myUu+VNAOBN0lOs339tpk1RLHTY32j2N8EbgKGkswtulHSOVkHZmbNr9+fYgOfAcZExKsAkr4DzAcuyTIwM2tuySh23lFUV0+CXNZjv0HUmFxpZlZTji3DelVbrOIKkj7HV4FFkmalzw8nGck2M9sgBc+PVVuQ3SPVi4Bflrw+J7twzKyV9NsWpBe3NbMsNUUfpKQRwHeAUcAm3a9HxC4ZxmVmLaDoLch65jReB/yIJOFPAG4BpmUYk5m1AAkGSnVveagnQW4WEbMAIuLxiDiPdMkgM7MN0cibdmWhnmk+byhpBz8u6UtAF1DzZjdmZrUU/RS7ngT5VWBz4AySvsitgM9nGZSZtYaC58e6Fqt4IH34Em8vmmtmtkGEGroeZBaqTRS/lXVverNWRHwik4jMrDXk2LdYr2otyKv6LIrUQ0v+xNbjJvd1tX3uhbl9/qfNRSt8l7Zh+m0fZETc05eBmFnrKfraifWuB2lm1lCiH7cgzcyy1u8vNewmaeOIeCPLYMysdfSHWy7Us6L4fpL+ADyWPt9L0n9kHpmZNb0Bqn/LJb469vkecBTwHEBE/B5famhmDdAMlxoOiIhlPTpT12QUj5m1iGS5s2KfYteTIJ+WtB8QkgYCXwEezTYsM2sFzTDN58skp9k7AH8Bfp2+Zma2QQregKzrWuxngRP6IBYzayFSP74Wu5ukayhzTXZEnJZJRGbWMgqeH+s6xf51yeNNgL8Hns4mHDNrJQWfBlnXKfbNpc8l3QD8KrOIzKwliOJPFF+fSw2HA8MaHYiZtZgcJ4DXq54+yBd4uw9yAPA88I0sgzKz1iCKnSGrJsj0XjR7kdyHBuCtiKi4iK6ZWb36w32xq87TTJPhrRGxJt2cHM2sYZrhWuwHJe2beSRm1nIk1b3lodo9aQZFxGrg74AvSHoceIWkZRwR4aRpZuutP5xiV+uDfBDYFzi2j2Ixs1bS4FV6JI0HrgQGAv8ZEZdW2O9TwHRgXETMq1ZmtQQpgIh4fP3CNTOrrlGXGqYL6UwBDgM6gbmSZkTE4h77bQGcATywbill4qvy3raSzqq0redxFMLVF5zEsnsuYd70c/MOJXN3z7qLPUfvyujdRnL5ZWX/Qe33/H32T92n2A0apNkP6IiIJyJiFTANmFhmv4uBy4DX64mxWoIcCGwObFFh67dumDmHiadPyTuMzK1Zs4Yzzzid22beyUMLFzN92k0sWby49gf7GX+f/ZUYqPo3YIikeSVb6XoQbbzzEujO9LW3a5P2AbaPiNvrjbDaKfYzEXFRvQX1J7MXPM4O739P3mFkbu6DDzJixEiG77QTAJOOP4HbZ97G7qNG5RxZY/n77J/fZ3JXw159ZGVEjK1SXE9rpyVKGgBcAZzSmwqrtSALPr5ktSxf3kV7+/Zrn7e1tdPV1VXlE1ZkTfd99uL0uo5T7E5g+5Ln7cDykudbAHsA90p6CvgQMENSpYQLVE+Qh9QMqQpJ20v6jaQlkhZJ+ucNKc96r9y8/qLfh9gqa8bvc0C6JmQ9Ww1zgZ0lDZc0mGQN2xndb0bEixExJCJ2jIgdgTnAMes9ih0Rz9d7kBWsBs6OiAXpyNF8Sb/qOapk2Wlra6ez8+1uma6uToYOHZpjRLYhmu37XI9T7IoiYrWkycAskvGTayNikaSLgHkRMaN6CeWtz2o+dYmIZ4Bn0scvSVpC0mnqBNlHxo4bR0fHYzz15JMMbWtj+s3TuO6GG/MOy9ZTM36fjVxRPCLuAO7o8dr5FfY9uJ4y++SeOZJ2BPahzNwjSad1j0rF6tf6Ihyuv+QU7r3+bHYZth0dd13Mycfu3yf19rVBgwZxxZVXcfSRH2fvD+zOJycdx6jRo/MOq+H8ffZfzXDb1w0iaXPg58CZEfG3nu9HxFRgKsCAzd7bJ4thnHzOdX1RTSGMn3AE4ycckXcYmfL32T+J5rir4XqTtBFJcvxpRPwiy7rMrJ9R8QeZMkuQ6VqSPwSWRMR3s6rHzPqvYqfHbFu4BwKfBT4m6eF0a45zAzPbYILeXknT57Icxb6P4v8DYWY5KvgZdvaDNGZm5eW3EG69nCDNLBctP4ptZlaNW5BmZhUUOz06QZpZXlp5HqSZWTXugzQzq8ItSDOzCvrzbV/NzDKTnGIXO0M6QZpZbgp+hu0EaWZ5EXIL0sysPLcgzczKcB+kmVklOd5KoV5OkGaWGydIM7MKPEhjZlaG8ERxM7OKGnlf7Cw4QZpZbnyKbWZWhk+xzcwq8pU0ZmbleR6kmVllBc+PxUqQ++y+A7MfuCrvMMx6Zetxk/MOIXNvLP1Tw8tM+iCLnSILlSDNrLUUOz06QZpZngqeIZ0gzSw3PsU2M6ug2OnRCdLM8lTwDOkEaWa5EL7U0MysPE8UNzOrrOD5kQF5B2BmLUy92GoVJY2XtFRSh6RvlHn/LEmLJS2UdI+kYbXKdII0s5yoV/9VLUkaCEwBJgCjgBMljeqx20PA2IjYE/gZcFmtCJ0gzSw3Uv1bDfsBHRHxRESsAqYBE0t3iIjfRMSr6dM5QHutQp0gzSwXvTm7TvPjEEnzSrbTSoprA54ued6ZvlbJqcCdtWL0II2Z5Ua9G8ZeGRFjKxVV5rWoUOdngLHAQbUqdII0s9w0cJpPJ7B9yfN2YPm69elQ4JvAQRHxRq1CfYptZrlp4CD2XGBnScMlDQZOAGa8oy5pH+AHwDER8Ww98TlBmlk+1qMTspKIWA1MBmYBS4BbImKRpIskHZPudjmwOTBd0sOSZlQobi2fYptZbhp5qWFE3AHc0eO180seH9rbMp0gzSwXwpcamplVVPD86ARpZjkqeIZ0gjSz3BR9ubOWHcW+e9Zd7Dl6V0bvNpLLL7s073Ay0wrHefUFJ7HsnkuYN/3cvEPJVDMe5wDVv+USXz7V5mvNmjWcecbp3DbzTh5auJjp025iyeLFeYfVcK1ynDfMnMPE06fkHUbmmvI4GzgRMgstmSDnPvggI0aMZPhOOzF48GAmHX8Ct8+8Le+wGq5VjnP2gsd5/sVXa+/YzzXbcXavKN6I1Xyy0pIJcvnyLtrb374qqa2tna6urhwjykarHKf1U71YySev6UCZDdJI2gT4LbBxWs/PIuKCrOrrjYh1r2Hv5UXz/UKrHKf1X0X/NWY5iv0G8LGIeFnSRsB9ku6MiDkZ1lmXtrZ2OjvfXhmpq6uToUOH5hhRNlrlOK0fK3iGzOwUOxIvp083Sreyyw/1tbHjxtHR8RhPPfkkq1atYvrN0zjyqGNqf7CfaZXjtP6qcSuKZyXTPkhJAyU9DDwL/CoiHiizz2ndC2CuWLkiy3DWGjRoEFdceRVHH/lx9v7A7nxy0nGMGj26T+ruS61ynNdfcgr3Xn82uwzbjo67LubkY/fPO6RMNONxFr0PUuX6qRpeifRu4FbgKxHxx0r7jRkzNmY/MC/zeKxvbD1uct4hWIO8sfQW3nr12YamqT33HhMzfj277v2Hb7vp/CoL5maiT0axI+KvwL3A+L6oz8z6iVadBylp27TliKRNgUOBR7Kqz8z6nwFS3VseshzFfj9wfXo7xgEkC1jenmF9ZtbPFHwQO7sEGRELgX2yKt/M+rkcB1/q5dV8zCxHxc6QTpBmlguvKG5mVkXB86MTpJnlxy1IM7MKir6iuBOkmeWn2PnRCdLM8lPw/OgEaWb5kMjtCpl6OUGaWX6KnR+dIM0sPwXPj06QZpafgp9hO0GaWV7yWym8Xk6QZpaL/nCpYUve9tXMrB5uQZpZboregnSCNLPcuA/SzKyMZKJ43lFU5wRpZvlxgjQzK8+n2GZmFRR9kMbTfMwsN428Lbak8ZKWSuqQ9I0y728s6eb0/Qck7VirTCdIM8tPgzJkenvpKcAEYBRwoqRRPXY7FXghIkYCVwD/Vis8J0gzy4168V8N+wEdEfFERKwCpgETe+wzEbg+ffwz4BCp+kl+ofogFyyYv3LTjbSsj6sdAqzs4zrz0ArH2QrHCPkc57BGF/jQgvmzNhusIb34yCaS5pU8nxoRU9PHbcDTJe91Ah/s8fm1+0TEakkvAttQ5W9ZqAQZEdv2dZ2S5kXE2L6ut6+1wnG2wjFC8xxnRIxvYHHlWoKxHvu8g0+xzawZdALblzxvB5ZX2kfSIGAr4PlqhTpBmlkzmAvsLGm4pMHACcCMHvvMAE5OH38K+O+IqNqCLNQpdk6m1t6lKbTCcbbCMULrHGfd0j7FycAsYCBwbUQsknQRMC8iZgA/BG6Q1EHScjyhVrmqkUDNzFqWT7HNzCpwgjQzq8AJ0sysgpZLkJJ2lbS/pI3Sy5OaWrMfo6SRksZK2jjvWLIkabSkgyRtk3csraSlBmkkfQL4V6Ar3eYB10XE33INLAOSdomIR9PHAyNiTd4xNZqko0i+z+eAPwMXdB9zM5E0geS64SeAjYBTI+LP+UbVGlqmBSlpI+B4kh/XIcBtJJNG/0XSlrkG12Bp4nhY0o0AEbGm2VqSkg4A/g9wckR8FHgBWGcFl/5O0sHAlcA/RsSxwCpgj1yDaiEtkyBTWwI7p49vBW4HBgOfrnXRen8h6V3AZOBMYJWkn0BzJkng0oh4KH18AfCeJjzV/gvwxYh4UNL7SK4vnizpB5I+1Sy/26JqmQQZEW8C3wU+IenDEfEWcB/wMPB3uQbXQBHxCvB54EbgayQX+K9NknnG1mAPAL+Atf2sG5MsqLBl+lpT9NVFxJKI+E369FTg+2lLcg4wiWThCstIyyTI1O+Au4HPSvpIRKyJiBuBocBe+YbWOBGxPCJejoiVwBeBTbuTpKR9Je2Wb4QbLv3uuvuOBfwVeD4iVkg6Cfi2pE3zi7DxIuI7EfHt9PGPgC145/XH1mAtdalhRLwu6ackK3ickyaKN4DtgGdyDS4jEfGcpC8Cl0t6hOQyrI/mHFZDRcRq4GVJT0u6BDgcOCUiXss5tIaRpNLrhiV9kuR323NBBmuglkqQABHxgqRrgMUkravXgc9ExF/yjSw7EbFS0kKS1ZYPi4jOvGNqpLQfbiPgw+n/D4mIx/KNqrG6k2Pax/oZ4CzgeI9mZ6ulpvn0lPZdRdof2bQkbQ3cApwdEQvzjicrkk4B5kbEorxjyUo6G+Mw4PGIWJp3PM2upRNkK5G0SUS8nnccWep5Gmq2oZwgzcwqaLVRbDOzujlBmplV4ARpZlaBE6SZWQVOkE1C0hpJD0v6o6TpkjbbgLIOlnR7+vgYSRUXgZD0bkn/tB51XCjpa/W+3mOf6yR9qhd17Sjpj72N0cwJsnm8FhF7R8QeJCu+fKn0TSV6/X1HxIyIuLTKLu8Gep0gzfoDJ8jm9DtgZNpyWiLp+8ACYHtJh0u6X9KCtKW5OYCk8ZIekXQf8InugiSdIumq9PF2km6V9Pt0OwC4FBiRtl4vT/f7uqS5khZK+lZJWd+UtFTSr4Fdax2EpC+k5fxe0s97tIoPlfQ7SY+my7shaaCky0vq/uKG/iGttTlBNhklN0SfAPwhfWlX4McRsQ/wCnAecGhE7EuyYPBZkjYBrgGOJrlc730Viv8e8D8RsRewL7CIZA3Gx9PW69clHU6ypNx+wN7AGEkfkTSG5Dab+5Ak4HF1HM4vImJcWt8SktVsuu0IHAQcCVydHsOpwIsRMS4t/wuShtdRj1lZLXctdhPbVNLD6ePfkdwDeCiwLCLmpK9/CBgFzE6XERwM3A/sBjzZff1yuvLPaWXq+BjwD7B26bQX08sYSx2ebt3rNG5OkjC3AG6NiFfTOnre1L2cPSR9m+Q0fnOSex53uyW9RPQxSU+kx3A4sGdJ/+RWad1Nt8q49Q0nyObxWkTsXfpCmgRfKX0J+FVEnNhjv71JVjhqBAGXRMQPetRx5nrUcR1wbET8Pr3O+uCS93qWFWndX4mI0kSKpB17Wa8Z4FPsVjMHOFDSSABJm0naBXgEGC5pRLrfiRU+fw/w5fSzA5XcquIlktZht1nA50v6NtskvRf4LfD3kjaVtAXJ6XwtWwDPpAs0nNTjvUmSBqQx7wQsTev+cro/knZRssK62XpxC7KFpIvJngLcpLdvTXB9MLOGAAAAlklEQVReRDwq6TTgl5JWkqy0Xu6+J/8MTJV0KrAG+HJE3C9pdjqN5s60H3J34P60BfsyyXJyCyTdTLKC+zKSboBa/jfJyuHLSPpUSxPxUuB/SNZE/FK61ud/kvRNLkiXQFsBHFvfX8dsXV6swsysAp9im5lV4ARpZlaBE6SZWQVOkGZmFThBmplV4ARpZlaBE6SZWQX/HyklrMslyATDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(x_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test,axis = 1) \n",
    "\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR With Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "nb_classes = 4\n",
    "lmda = 0.01\n",
    "\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size, epochs=nb_epoch,\n",
    "                    verbose=1, validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=2)\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 10\n",
      "Train on 76 samples, validate on 12 samples\n",
      "Epoch 1/10\n",
      "76/76 [==============================] - 2s 29ms/step - loss: 1.6047 - acc: 0.2763 - val_loss: 1.4658 - val_acc: 0.4167\n",
      "Epoch 2/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.3884 - acc: 0.3289 - val_loss: 1.3951 - val_acc: 0.4167\n",
      "Epoch 3/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.3215 - acc: 0.3684 - val_loss: 1.3725 - val_acc: 0.3333\n",
      "Epoch 4/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.2806 - acc: 0.4342 - val_loss: 1.3583 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.2572 - acc: 0.5132 - val_loss: 1.3640 - val_acc: 0.1667\n",
      "Epoch 6/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.2333 - acc: 0.5263 - val_loss: 1.3494 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.2225 - acc: 0.5395 - val_loss: 1.3429 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.2066 - acc: 0.5263 - val_loss: 1.3523 - val_acc: 0.1667\n",
      "Epoch 9/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.1961 - acc: 0.5526 - val_loss: 1.3492 - val_acc: 0.1667\n",
      "Epoch 10/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.1871 - acc: 0.5658 - val_loss: 1.3498 - val_acc: 0.1667\n",
      "Running Fold 2 / 10\n",
      "Train on 76 samples, validate on 12 samples\n",
      "Epoch 1/10\n",
      "76/76 [==============================] - 2s 29ms/step - loss: 1.9243 - acc: 0.2237 - val_loss: 1.6402 - val_acc: 0.1667\n",
      "Epoch 2/10\n",
      "76/76 [==============================] - 0s 5ms/step - loss: 1.6303 - acc: 0.2632 - val_loss: 1.5735 - val_acc: 0.0833\n",
      "Epoch 3/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.4932 - acc: 0.3289 - val_loss: 1.5516 - val_acc: 0.1667\n",
      "Epoch 4/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.4106 - acc: 0.3553 - val_loss: 1.5174 - val_acc: 0.3333\n",
      "Epoch 5/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.3569 - acc: 0.4079 - val_loss: 1.4388 - val_acc: 0.4167\n",
      "Epoch 6/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.3195 - acc: 0.4211 - val_loss: 1.4108 - val_acc: 0.4167\n",
      "Epoch 7/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.2915 - acc: 0.4605 - val_loss: 1.3979 - val_acc: 0.5833\n",
      "Epoch 8/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.2727 - acc: 0.4737 - val_loss: 1.3348 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.2576 - acc: 0.5000 - val_loss: 1.3028 - val_acc: 0.4167\n",
      "Epoch 10/10\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 1.2453 - acc: 0.5000 - val_loss: 1.3142 - val_acc: 0.5833\n",
      "Running Fold 3 / 10\n",
      "Train on 80 samples, validate on 8 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 1.6339 - acc: 0.3125 - val_loss: 1.4263 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.5157 - acc: 0.3625 - val_loss: 1.4265 - val_acc: 0.3750\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4279 - acc: 0.3625 - val_loss: 1.4262 - val_acc: 0.3750\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3601 - acc: 0.4250 - val_loss: 1.4648 - val_acc: 0.3750\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3112 - acc: 0.4125 - val_loss: 1.4647 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2733 - acc: 0.4250 - val_loss: 1.4624 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2425 - acc: 0.4500 - val_loss: 1.4217 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2249 - acc: 0.5000 - val_loss: 1.4100 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2088 - acc: 0.4875 - val_loss: 1.4316 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1943 - acc: 0.5000 - val_loss: 1.4132 - val_acc: 0.5000\n",
      "Running Fold 4 / 10\n",
      "Train on 80 samples, validate on 8 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 1.9532 - acc: 0.2125 - val_loss: 1.9197 - val_acc: 0.1250\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.5709 - acc: 0.3250 - val_loss: 1.7740 - val_acc: 0.1250\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.4496 - acc: 0.328 - 0s 5ms/step - loss: 1.4408 - acc: 0.3250 - val_loss: 1.6887 - val_acc: 0.1250\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3751 - acc: 0.3750 - val_loss: 1.6178 - val_acc: 0.1250\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3300 - acc: 0.4250 - val_loss: 1.5566 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2976 - acc: 0.4625 - val_loss: 1.5173 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2713 - acc: 0.4625 - val_loss: 1.4880 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2534 - acc: 0.5000 - val_loss: 1.4619 - val_acc: 0.1250\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2390 - acc: 0.5000 - val_loss: 1.4337 - val_acc: 0.1250\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2252 - acc: 0.5125 - val_loss: 1.4284 - val_acc: 0.1250\n",
      "Running Fold 5 / 10\n",
      "Train on 80 samples, validate on 8 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 28ms/step - loss: 1.8030 - acc: 0.3250 - val_loss: 1.5264 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5431 - acc: 0.4000 - val_loss: 1.4279 - val_acc: 0.3750\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4072 - acc: 0.4625 - val_loss: 1.3913 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.3297 - acc: 0.4375 - val_loss: 1.3587 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2818 - acc: 0.4375 - val_loss: 1.3384 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2567 - acc: 0.4500 - val_loss: 1.3377 - val_acc: 0.6250\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2402 - acc: 0.4375 - val_loss: 1.3236 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2248 - acc: 0.4875 - val_loss: 1.3229 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2123 - acc: 0.5250 - val_loss: 1.3225 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2039 - acc: 0.5000 - val_loss: 1.3121 - val_acc: 0.5000\n",
      "Running Fold 6 / 10\n",
      "Train on 80 samples, validate on 8 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 28ms/step - loss: 1.5289 - acc: 0.3000 - val_loss: 1.3109 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4225 - acc: 0.3750 - val_loss: 1.2457 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3672 - acc: 0.3875 - val_loss: 1.2337 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3277 - acc: 0.4750 - val_loss: 1.2432 - val_acc: 0.3750\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2996 - acc: 0.4500 - val_loss: 1.2563 - val_acc: 0.1250\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2744 - acc: 0.4750 - val_loss: 1.2782 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2540 - acc: 0.4625 - val_loss: 1.3039 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2402 - acc: 0.5000 - val_loss: 1.3159 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2258 - acc: 0.5375 - val_loss: 1.3413 - val_acc: 0.1250\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2155 - acc: 0.5750 - val_loss: 1.3441 - val_acc: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 7 / 10\n",
      "Train on 80 samples, validate on 8 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 28ms/step - loss: 1.5691 - acc: 0.3750 - val_loss: 1.6529 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3663 - acc: 0.4250 - val_loss: 1.6965 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2817 - acc: 0.4375 - val_loss: 1.7195 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2379 - acc: 0.4750 - val_loss: 1.7394 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2148 - acc: 0.4875 - val_loss: 1.7399 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1948 - acc: 0.4875 - val_loss: 1.7465 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1782 - acc: 0.5375 - val_loss: 1.7559 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1661 - acc: 0.5125 - val_loss: 1.7585 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1556 - acc: 0.5250 - val_loss: 1.7744 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1447 - acc: 0.5375 - val_loss: 1.7888 - val_acc: 0.0000e+00\n",
      "Running Fold 8 / 10\n",
      "Train on 80 samples, validate on 8 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 28ms/step - loss: 1.7412 - acc: 0.3000 - val_loss: 2.4577 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4744 - acc: 0.4125 - val_loss: 2.0571 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3429 - acc: 0.4500 - val_loss: 1.8825 - val_acc: 0.1250\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2844 - acc: 0.4875 - val_loss: 1.8216 - val_acc: 0.1250\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2554 - acc: 0.5125 - val_loss: 1.8120 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2353 - acc: 0.5000 - val_loss: 1.7941 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2163 - acc: 0.4875 - val_loss: 1.8108 - val_acc: 0.3750\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1991 - acc: 0.5125 - val_loss: 1.7786 - val_acc: 0.3750\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1863 - acc: 0.5500 - val_loss: 1.7741 - val_acc: 0.3750\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1771 - acc: 0.5625 - val_loss: 1.7741 - val_acc: 0.5000\n",
      "Running Fold 9 / 10\n",
      "Train on 80 samples, validate on 8 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 28ms/step - loss: 1.9085 - acc: 0.2750 - val_loss: 1.3734 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5925 - acc: 0.2500 - val_loss: 1.2926 - val_acc: 0.3750\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4393 - acc: 0.3875 - val_loss: 1.2381 - val_acc: 0.3750\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3634 - acc: 0.3875 - val_loss: 1.2067 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.3154 - acc: 0.4500 - val_loss: 1.1549 - val_acc: 0.6250\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2918 - acc: 0.4250 - val_loss: 1.1180 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2713 - acc: 0.5000 - val_loss: 1.0828 - val_acc: 0.7500\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2587 - acc: 0.4500 - val_loss: 1.0622 - val_acc: 0.7500\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.2455 - acc: 0.4500 - val_loss: 1.0548 - val_acc: 0.6250\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.2360 - acc: 0.4625 - val_loss: 1.0552 - val_acc: 0.6250\n",
      "Running Fold 10 / 10\n",
      "Train on 80 samples, validate on 8 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 3s 39ms/step - loss: 1.7817 - acc: 0.3750 - val_loss: 1.4231 - val_acc: 0.3750\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.5156 - acc: 0.3750 - val_loss: 1.2034 - val_acc: 0.6250\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.3959 - acc: 0.3875 - val_loss: 1.0789 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.3389 - acc: 0.4000 - val_loss: 1.0461 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.3001 - acc: 0.4250 - val_loss: 1.0344 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.2808 - acc: 0.4500 - val_loss: 1.0100 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.2651 - acc: 0.4625 - val_loss: 0.9781 - val_acc: 0.6250\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.2511 - acc: 0.4500 - val_loss: 0.9384 - val_acc: 0.7500\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.2443 - acc: 0.4625 - val_loss: 0.9112 - val_acc: 0.7500\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.2393 - acc: 0.4500 - val_loss: 0.9130 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "n_folds = 10\n",
    "\n",
    "labels = df['date'].values\n",
    "data = x\n",
    "\n",
    "labels[labels == '2017.11.09'] = 0\n",
    "labels[labels == '2017.11.17'] = 1\n",
    "labels[labels == '2017.11.22'] = 2\n",
    "labels[labels == '2017.11.23'] = 3\n",
    "\n",
    "one_hot = np_utils.to_categorical(labels, nb_classes) \n",
    "\n",
    "skf = StratifiedKFold(df['date'].values, n_folds=n_folds, shuffle=True)\n",
    "avg_acc = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "    model = None # Clearing the NN.\n",
    "    model = build_logistic_model(input_dim, nb_classes, lmda)\n",
    "    \n",
    "    std = np.std(data[train],0)\n",
    "    mean = np.mean(data[train],0)\n",
    "    \n",
    "    x_train = (data[train]-mean)/std\n",
    "    x_test = (data[test] - mean)/std        \n",
    "    \n",
    "    avg_acc += train_and_evaluate_model(model, x_train, one_hot[train], x_test, one_hot[test])\n",
    "    \n",
    "    # Predict the values from the validation dataset\n",
    "    Y_pred = model.predict(x_test)\n",
    "    # Convert predictions classes to one hot vectors \n",
    "    Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "    # Convert validation observations to one hot vectors\n",
    "    Y_true = np.argmax(one_hot[test],axis = 1) \n",
    "    \n",
    "    y_true.extend(Y_true)\n",
    "    y_pred.extend(Y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  0.38749999850988387\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Accuracy: \", avg_acc/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(np.array(y_true), np.array(y_pred)) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search on Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Test dataset size is wrong?\n",
    "\n",
    "Also: This takes a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "[CV] batch_size=4, epochs=10, lmda=0.001 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 12.2435 - acc: 0.2405\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 12.2435 - acc: 0.2405\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 12.2435 - acc: 0.2405\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 12.2435 - acc: 0.2405\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 12.2435 - acc: 0.2405\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 924us/step - loss: 12.2435 - acc: 0.2405\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 905us/step - loss: 12.2435 - acc: 0.24050s - loss: 12.2164 - acc: 0.24\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 886us/step - loss: 12.2435 - acc: 0.2405\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 12.2435 - acc: 0.2405\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 879us/step - loss: 12.2435 - acc: 0.2405\n",
      "18/18 [==============================] - 0s 7ms/step\n",
      "158/158 [==============================] - 0s 468us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.001, total=   2.2s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.001 .............................\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 0s 3ms/step - loss: 12.1404 - acc: 0.2468\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 904us/step - loss: 12.1404 - acc: 0.2468\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 848us/step - loss: 12.1404 - acc: 0.2468\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 943us/step - loss: 12.1404 - acc: 0.2468\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 12.1404 - acc: 0.2468\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 999us/step - loss: 12.1404 - acc: 0.2468\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 879us/step - loss: 12.1404 - acc: 0.2468\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 876us/step - loss: 12.1404 - acc: 0.2468\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 915us/step - loss: 12.1404 - acc: 0.2468\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 938us/step - loss: 12.1404 - acc: 0.2468\n",
      "18/18 [==============================] - 0s 7ms/step\n",
      "158/158 [==============================] - 0s 449us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.001, total=   2.0s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.001 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 12.0400 - acc: 0.2532\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 12.0400 - acc: 0.2532\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 873us/step - loss: 12.0400 - acc: 0.2532TA: 0s - loss: 10.5799 - acc: 0.3438\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 892us/step - loss: 12.0400 - acc: 0.2532\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 987us/step - loss: 12.0400 - acc: 0.2532\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 905us/step - loss: 12.0400 - acc: 0.2532\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 892us/step - loss: 12.0400 - acc: 0.2532\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 936us/step - loss: 12.0400 - acc: 0.2532\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 917us/step - loss: 12.0400 - acc: 0.2532\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 867us/step - loss: 12.0400 - acc: 0.2532\n",
      "18/18 [==============================] - 0s 7ms/step\n",
      "158/158 [==============================] - 0s 538us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.001, total=   2.0s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.001 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 12.0394 - acc: 0.2532\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 866us/step - loss: 12.0394 - acc: 0.2532\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 12.0394 - acc: 0.2532\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 999us/step - loss: 12.0394 - acc: 0.2532\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 961us/step - loss: 12.0394 - acc: 0.2532\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 974us/step - loss: 12.0394 - acc: 0.2532\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 999us/step - loss: 12.0394 - acc: 0.2532\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 911us/step - loss: 12.0393 - acc: 0.2532\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 974us/step - loss: 12.0393 - acc: 0.2532\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 930us/step - loss: 12.0393 - acc: 0.2532\n",
      "18/18 [==============================] - 0s 8ms/step\n",
      "158/158 [==============================] - 0s 459us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.001, total=   2.1s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.001 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 12.0384 - acc: 0.2532\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 12.0384 - acc: 0.2532\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 936us/step - loss: 12.0384 - acc: 0.2532\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 12.0384 - acc: 0.2532\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 917us/step - loss: 12.0384 - acc: 0.2532\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 955us/step - loss: 12.0384 - acc: 0.2532\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 987us/step - loss: 12.0384 - acc: 0.2532\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 917us/step - loss: 12.0384 - acc: 0.2532\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 911us/step - loss: 12.0384 - acc: 0.2532\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 905us/step - loss: 12.0384 - acc: 0.2532\n",
      "18/18 [==============================] - 0s 8ms/step\n",
      "158/158 [==============================] - 0s 481us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.001, total=   2.2s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.001 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 11.7330 - acc: 0.2722\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 892us/step - loss: 11.7330 - acc: 0.2722\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 924us/step - loss: 11.7330 - acc: 0.2722\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 955us/step - loss: 11.7330 - acc: 0.2722\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 974us/step - loss: 11.7329 - acc: 0.2722\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 930us/step - loss: 11.7329 - acc: 0.2722\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 854us/step - loss: 11.7329 - acc: 0.2722\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 924us/step - loss: 11.7329 - acc: 0.2722\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 968us/step - loss: 11.7329 - acc: 0.2722\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 949us/step - loss: 11.7329 - acc: 0.2722\n",
      "18/18 [==============================] - 0s 8ms/step\n",
      "158/158 [==============================] - 0s 462us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.001, total=   2.1s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.001 .............................\n",
      "Epoch 1/10\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 11.8628 - acc: 0.2642\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 0s 861us/step - loss: 11.8628 - acc: 0.2642\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 0s 867us/step - loss: 11.8628 - acc: 0.2642\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 0s 905us/step - loss: 11.8628 - acc: 0.2642\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 0s 937us/step - loss: 11.8628 - acc: 0.2642\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - 0s 930us/step - loss: 11.8628 - acc: 0.2642\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 0s 930us/step - loss: 11.8628 - acc: 0.2642\n",
      "Epoch 8/10\n",
      "159/159 [==============================] - 0s 1ms/step - loss: 11.8628 - acc: 0.2642\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 0s 893us/step - loss: 11.8628 - acc: 0.2642\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 0s 918us/step - loss: 11.8628 - acc: 0.2642\n",
      "17/17 [==============================] - 0s 10ms/step\n",
      "159/159 [==============================] - 0s 509us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.001, total=   2.1s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.001 .............................\n",
      "Epoch 1/10\n",
      "159/159 [==============================] - 1s 3ms/step - loss: 12.2671 - acc: 0.2390\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 0s 911us/step - loss: 12.2671 - acc: 0.2390\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 0s 918us/step - loss: 12.2671 - acc: 0.2390\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 0s 899us/step - loss: 12.2671 - acc: 0.2390TA: 0s - loss: 11.8210 - acc: 0.26\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 0s 981us/step - loss: 12.2671 - acc: 0.2390\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - 0s 1ms/step - loss: 12.2671 - acc: 0.2390\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 0s 867us/step - loss: 12.2671 - acc: 0.2390\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 880us/step - loss: 12.2671 - acc: 0.2390\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 0s 842us/step - loss: 12.2671 - acc: 0.2390\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 0s 893us/step - loss: 12.2671 - acc: 0.2390\n",
      "17/17 [==============================] - 0s 10ms/step\n",
      "159/159 [==============================] - 0s 515us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.001, total=   2.1s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.001 .............................\n",
      "Epoch 1/10\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 11.6600 - acc: 0.2767\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 0s 849us/step - loss: 11.6600 - acc: 0.2767\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 0s 886us/step - loss: 11.6600 - acc: 0.2767\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 0s 955us/step - loss: 11.6600 - acc: 0.2767\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 0s 842us/step - loss: 11.6600 - acc: 0.2767\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - 0s 905us/step - loss: 11.6600 - acc: 0.2767\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 0s 930us/step - loss: 11.6600 - acc: 0.2767\n",
      "Epoch 8/10\n",
      "159/159 [==============================] - 0s 899us/step - loss: 11.6600 - acc: 0.2767\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 0s 874us/step - loss: 11.6600 - acc: 0.2767\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 0s 798us/step - loss: 11.6600 - acc: 0.2767\n",
      "17/17 [==============================] - 0s 11ms/step\n",
      "159/159 [==============================] - 0s 459us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.001, total=   2.1s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.001 .............................\n",
      "Epoch 1/10\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 12.0656 - acc: 0.2516\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 0s 855us/step - loss: 12.0656 - acc: 0.2516\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 0s 855us/step - loss: 12.0656 - acc: 0.2516\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 0s 905us/step - loss: 12.0656 - acc: 0.2516\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 0s 949us/step - loss: 12.0656 - acc: 0.2516\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - 0s 955us/step - loss: 12.0655 - acc: 0.2516\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 0s 930us/step - loss: 12.0655 - acc: 0.2516\n",
      "Epoch 8/10\n",
      "159/159 [==============================] - 0s 930us/step - loss: 12.0655 - acc: 0.2516\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 0s 955us/step - loss: 12.0655 - acc: 0.2516\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 0s 918us/step - loss: 12.0655 - acc: 0.2516\n",
      "17/17 [==============================] - 0s 10ms/step\n",
      "159/159 [==============================] - 0s 446us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.001, total=   2.2s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.005 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 12.0392 - acc: 0.2532\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 930us/step - loss: 12.0392 - acc: 0.2532\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 961us/step - loss: 12.0392 - acc: 0.2532\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 860us/step - loss: 12.0392 - acc: 0.2532\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 12.0392 - acc: 0.2532\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 841us/step - loss: 12.0392 - acc: 0.2532\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 917us/step - loss: 12.0392 - acc: 0.2532\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 810us/step - loss: 12.0391 - acc: 0.2532\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 822us/step - loss: 12.0391 - acc: 0.2532\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 955us/step - loss: 12.0391 - acc: 0.2532\n",
      "18/18 [==============================] - 0s 10ms/step\n",
      "158/158 [==============================] - 0s 538us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.005, total=   2.2s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.005 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 11.9450 - acc: 0.2595\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 993us/step - loss: 11.9449 - acc: 0.2595\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 911us/step - loss: 11.9448 - acc: 0.2595\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 11.9447 - acc: 0.2595\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 911us/step - loss: 11.9447 - acc: 0.2595\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 879us/step - loss: 11.9446 - acc: 0.2595\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 11.9445 - acc: 0.2595\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 892us/step - loss: 11.9444 - acc: 0.2595\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 860us/step - loss: 11.9444 - acc: 0.2595\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 949us/step - loss: 11.9443 - acc: 0.2595\n",
      "18/18 [==============================] - 0s 10ms/step\n",
      "158/158 [==============================] - 0s 487us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.005, total=   2.2s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.005 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 12.1480 - acc: 0.2468\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 968us/step - loss: 12.1479 - acc: 0.2468\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 911us/step - loss: 12.1479 - acc: 0.2468\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 955us/step - loss: 12.1478 - acc: 0.2468\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 987us/step - loss: 12.1477 - acc: 0.2468\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 949us/step - loss: 12.1477 - acc: 0.2468\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 987us/step - loss: 12.1476 - acc: 0.2468\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 949us/step - loss: 12.1475 - acc: 0.2468\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 12.1475 - acc: 0.2468\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 974us/step - loss: 12.1474 - acc: 0.2468\n",
      "18/18 [==============================] - 0s 12ms/step\n",
      "158/158 [==============================] - 0s 500us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.005, total=   2.3s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.005 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 12.0492 - acc: 0.2532\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 886us/step - loss: 12.0491 - acc: 0.2532\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 886us/step - loss: 12.0490 - acc: 0.2532\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 911us/step - loss: 12.0489 - acc: 0.2532\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 886us/step - loss: 12.0488 - acc: 0.2532\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 854us/step - loss: 12.0487 - acc: 0.2532\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 12.0486 - acc: 0.2532\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 911us/step - loss: 12.0486 - acc: 0.2532\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 854us/step - loss: 12.0485 - acc: 0.2532\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 867us/step - loss: 12.0484 - acc: 0.2532\n",
      "18/18 [==============================] - 0s 13ms/step\n",
      "158/158 [==============================] - 0s 500us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.005, total=   2.2s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.005 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 12.3538 - acc: 0.2342\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 879us/step - loss: 12.3538 - acc: 0.2342\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 0s 873us/step - loss: 12.3537 - acc: 0.2342\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 829us/step - loss: 12.3536 - acc: 0.2342\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 873us/step - loss: 12.3535 - acc: 0.2342\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 835us/step - loss: 12.3534 - acc: 0.2342\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 917us/step - loss: 12.3534 - acc: 0.2342\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 12.3533 - acc: 0.2342\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 848us/step - loss: 12.3532 - acc: 0.2342\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 873us/step - loss: 12.3531 - acc: 0.2342\n",
      "18/18 [==============================] - 0s 13ms/step\n",
      "158/158 [==============================] - 0s 449us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.005, total=   2.2s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.005 .............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 12.0465 - acc: 0.2532\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 12.0465 - acc: 0.2532\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 911us/step - loss: 12.0464 - acc: 0.2532\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 961us/step - loss: 12.0463 - acc: 0.2532\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 961us/step - loss: 12.0463 - acc: 0.2532\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 892us/step - loss: 12.0462 - acc: 0.2532\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 924us/step - loss: 12.0461 - acc: 0.2532\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 867us/step - loss: 12.0461 - acc: 0.2532\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 867us/step - loss: 12.0460 - acc: 0.2532\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 936us/step - loss: 12.0459 - acc: 0.2532\n",
      "18/18 [==============================] - 0s 13ms/step\n",
      "158/158 [==============================] - 0s 430us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.005, total=   2.3s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.005 .............................\n",
      "Epoch 1/10\n",
      "159/159 [==============================] - 1s 4ms/step - loss: 12.0753 - acc: 0.2516\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 0s 930us/step - loss: 12.0752 - acc: 0.2516\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 0s 880us/step - loss: 12.0751 - acc: 0.2516\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 0s 867us/step - loss: 12.0751 - acc: 0.2516\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 0s 823us/step - loss: 12.0750 - acc: 0.2516\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - 0s 867us/step - loss: 12.0749 - acc: 0.2516\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 0s 886us/step - loss: 12.0748 - acc: 0.2516\n",
      "Epoch 8/10\n",
      "159/159 [==============================] - 0s 911us/step - loss: 12.0747 - acc: 0.2516\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 0s 874us/step - loss: 12.0746 - acc: 0.25160s - loss: 12.0999 - acc: 0.25\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 0s 930us/step - loss: 12.0745 - acc: 0.2516\n",
      "17/17 [==============================] - 0s 15ms/step\n",
      "159/159 [==============================] - 0s 453us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.005, total=   2.3s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.005 .............................\n",
      "Epoch 1/10\n",
      "159/159 [==============================] - 1s 5ms/step - loss: 12.2758 - acc: 0.2390\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 0s 1ms/step - loss: 12.2757 - acc: 0.2390\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 0s 1ms/step - loss: 12.2757 - acc: 0.2390\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 0s 974us/step - loss: 12.2756 - acc: 0.2390\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 0s 1ms/step - loss: 12.2755 - acc: 0.2390\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - ETA: 0s - loss: 12.0980 - acc: 0.25 - 0s 880us/step - loss: 12.2754 - acc: 0.2390\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 0s 861us/step - loss: 12.2754 - acc: 0.2390\n",
      "Epoch 8/10\n",
      "159/159 [==============================] - 0s 855us/step - loss: 12.2753 - acc: 0.2390\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 0s 899us/step - loss: 12.2752 - acc: 0.2390\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 0s 893us/step - loss: 12.2751 - acc: 0.2390\n",
      "17/17 [==============================] - 0s 14ms/step\n",
      "159/159 [==============================] - 0s 497us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.005, total=   2.5s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.005 .............................\n",
      "Epoch 1/10\n",
      "159/159 [==============================] - 1s 5ms/step - loss: 11.8686 - acc: 0.2642\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 0s 937us/step - loss: 11.8685 - acc: 0.2642\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 0s 867us/step - loss: 11.8685 - acc: 0.2642\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 0s 886us/step - loss: 11.8684 - acc: 0.2642\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 0s 880us/step - loss: 11.8683 - acc: 0.2642\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - 0s 968us/step - loss: 11.8683 - acc: 0.2642\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 0s 924us/step - loss: 11.8682 - acc: 0.2642\n",
      "Epoch 8/10\n",
      "159/159 [==============================] - 0s 937us/step - loss: 11.8681 - acc: 0.2642\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 0s 981us/step - loss: 11.8681 - acc: 0.2642\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 0s 886us/step - loss: 11.8680 - acc: 0.2642\n",
      "17/17 [==============================] - 0s 15ms/step\n",
      "159/159 [==============================] - 0s 497us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.005, total=   2.6s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.005 .............................\n",
      "Epoch 1/10\n",
      "159/159 [==============================] - 1s 5ms/step - loss: 12.1673 - acc: 0.2453\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 0s 823us/step - loss: 12.1673 - acc: 0.2453\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 0s 918us/step - loss: 12.1673 - acc: 0.2453\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 0s 911us/step - loss: 12.1672 - acc: 0.2453\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 0s 899us/step - loss: 12.1672 - acc: 0.2453\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - 0s 855us/step - loss: 12.1672 - acc: 0.2453\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 0s 880us/step - loss: 12.1672 - acc: 0.2453\n",
      "Epoch 8/10\n",
      "159/159 [==============================] - 0s 886us/step - loss: 12.1672 - acc: 0.2453\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 0s 886us/step - loss: 12.1671 - acc: 0.2453\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 0s 836us/step - loss: 12.1671 - acc: 0.2453\n",
      "17/17 [==============================] - 0s 15ms/step\n",
      "159/159 [==============================] - 0s 465us/step\n",
      "[CV] .............. batch_size=4, epochs=10, lmda=0.005, total=   2.4s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.01 ..............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 5ms/step - loss: 12.2529 - acc: 0.2405\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 886us/step - loss: 12.2527 - acc: 0.2405\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 12.2525 - acc: 0.2405\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 955us/step - loss: 12.2524 - acc: 0.2405\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 12.2522 - acc: 0.2405\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 892us/step - loss: 12.2520 - acc: 0.2405\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 961us/step - loss: 12.2518 - acc: 0.2405\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 12.2517 - acc: 0.2405\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 12.2515 - acc: 0.2405\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 0s 1ms/step - loss: 12.2514 - acc: 0.2405\n",
      "18/18 [==============================] - 0s 15ms/step\n",
      "158/158 [==============================] - 0s 443us/step\n",
      "[CV] ............... batch_size=4, epochs=10, lmda=0.01, total=   2.5s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.01 ..............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 5ms/step - loss: 11.9541 - acc: 0.2595\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 886us/step - loss: 11.9539 - acc: 0.2595\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 11.9536 - acc: 0.2595\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 892us/step - loss: 11.9533 - acc: 0.2595\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 917us/step - loss: 11.9530 - acc: 0.2595\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 987us/step - loss: 11.9527 - acc: 0.2595\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 0s 911us/step - loss: 11.9524 - acc: 0.2595\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 0s 898us/step - loss: 11.9522 - acc: 0.2595\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 0s 905us/step - loss: 11.9519 - acc: 0.2595\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 0s 892us/step - loss: 11.9517 - acc: 0.2595\n",
      "18/18 [==============================] - 0s 16ms/step\n",
      "158/158 [==============================] - 0s 487us/step\n",
      "[CV] ............... batch_size=4, epochs=10, lmda=0.01, total=   2.5s\n",
      "[CV] batch_size=4, epochs=10, lmda=0.01 ..............................\n",
      "Epoch 1/10\n",
      "158/158 [==============================] - 1s 5ms/step - loss: 12.0717 - acc: 0.2532\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 0s 924us/step - loss: 12.0712 - acc: 0.2532\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 0s 873us/step - loss: 12.0707 - acc: 0.2532\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 0s 968us/step - loss: 12.0701 - acc: 0.2532\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 0s 873us/step - loss: 12.0696 - acc: 0.2532\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 0s 911us/step - loss: 12.0691 - acc: 0.2532\n",
      "Epoch 7/10\n",
      "  4/158 [..............................] - ETA: 0s - loss: 12.1199 - acc: 0.2500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a98cd9a57354>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=compile_model,input_dim=input_dim, output_dim=nb_classes, optimizer='sgd')\n",
    "\n",
    "param_grid = dict(epochs=[10,20,30], batch_size=[4,8,16,32], lmda=[0.001,0.005,0.01,0.05,0.1])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, verbose=2)\n",
    "grid_result = grid.fit(data, one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = len(means)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "plt.bar(x, means)\n",
    "plt.xticks(x, params)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
