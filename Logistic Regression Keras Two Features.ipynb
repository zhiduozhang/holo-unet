{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet_ml import Experiment\n",
    "# experiment = Experiment(api_key=\"xktj4EX0zB8YcQ3BEaFwOQYpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Duo\\Anaconda3\\envs\\fastai\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Duo\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\train\\\\\"\n",
    "train_csv = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\train\\\\ids.csv\"\n",
    "test_path = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\test\\\\\"\n",
    "test_csv = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\test\\\\ids.csv\"\n",
    "\n",
    "dest_path = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\\"\n",
    "\n",
    "csv = \"I:\\\\Honours-Project\\\\data\\\\sorted\\\\agg\\\\ids.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv,index_col=0).sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample to balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow_rate\n",
      "1800    116\n",
      "3600    116\n",
      "7200    116\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ds_count = min(df.groupby('flow_rate').size())\n",
    "ds_flow = df.groupby('flow_rate').size().idxmin()\n",
    "\n",
    "samples = df[df['flow_rate']==1800].sample(n=ds_count)\n",
    "samples.append(df[df['flow_rate']==3600].sample(n=ds_count))\n",
    "samples.append(df[df['flow_rate']==7200].sample(n=ds_count))\n",
    "\n",
    "samples = pd.concat([df[df['flow_rate']==i].sample(n=ds_count) for i in (1800,3600,7200)],ignore_index=True)\n",
    "\n",
    "df = samples.sample(frac=1)\n",
    "\n",
    "print(samples.groupby('flow_rate').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_volume'] = df['volume'].apply(np.log)\n",
    "\n",
    "x = df.drop(columns=['name','date','flow_rate','source','moments','inertia_tensor'])\n",
    "\n",
    "# Drop catagorical features\n",
    "mat = x.drop(columns=['euler_number', 'num_peaks', 'cluster'])\n",
    "\n",
    "a = mat.as_matrix()\n",
    "# x = standardize = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>flow_rate</th>\n",
       "      <th>source</th>\n",
       "      <th>peak_val</th>\n",
       "      <th>area</th>\n",
       "      <th>volume</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>euler_number</th>\n",
       "      <th>extent</th>\n",
       "      <th>mean_intensity</th>\n",
       "      <th>moments</th>\n",
       "      <th>orientation</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>inertia_tensor</th>\n",
       "      <th>cluster</th>\n",
       "      <th>num_peaks</th>\n",
       "      <th>log_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>118_flipped</td>\n",
       "      <td>2017.11.22</td>\n",
       "      <td>7200</td>\n",
       "      <td>34.20 s.tifheightmap.mat</td>\n",
       "      <td>8.531226</td>\n",
       "      <td>3004</td>\n",
       "      <td>16022.993157</td>\n",
       "      <td>0.749848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.661237</td>\n",
       "      <td>5.333886</td>\n",
       "      <td>[[3.00400000e+03 1.05119000e+05 4.62409500e+06...</td>\n",
       "      <td>-1.031204</td>\n",
       "      <td>219.379726</td>\n",
       "      <td>[[216.68422529 -91.61993795]\\r\\n [-91.61993795...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.681780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>73</td>\n",
       "      <td>2017.11.17</td>\n",
       "      <td>7200</td>\n",
       "      <td>5.65 s.tifheightmap.mat</td>\n",
       "      <td>8.775024</td>\n",
       "      <td>12462</td>\n",
       "      <td>54832.268548</td>\n",
       "      <td>0.984844</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606778</td>\n",
       "      <td>4.399957</td>\n",
       "      <td>[[1.24620000e+04 2.17597800e+06 4.54427324e+08...</td>\n",
       "      <td>-1.565982</td>\n",
       "      <td>721.019336</td>\n",
       "      <td>[[ 179.9360201   -27.90561956]\\r\\n [ -27.90561...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.912034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4_flipped</td>\n",
       "      <td>2017.11.09</td>\n",
       "      <td>1800</td>\n",
       "      <td>chip2_06heightmap.mat</td>\n",
       "      <td>4.629928</td>\n",
       "      <td>1929</td>\n",
       "      <td>3566.980825</td>\n",
       "      <td>0.871295</td>\n",
       "      <td>1</td>\n",
       "      <td>0.562063</td>\n",
       "      <td>1.849135</td>\n",
       "      <td>[[1.92900000e+03 6.37560000e+04 2.84693000e+06...</td>\n",
       "      <td>1.543902</td>\n",
       "      <td>209.095454</td>\n",
       "      <td>[[ 92.6177739    7.82965009]\\r\\n [  7.82965009...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.179475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>11_flipped</td>\n",
       "      <td>2017.11.09</td>\n",
       "      <td>3600</td>\n",
       "      <td>chip3_01heightmap.mat</td>\n",
       "      <td>14.033560</td>\n",
       "      <td>175</td>\n",
       "      <td>708.182643</td>\n",
       "      <td>0.665178</td>\n",
       "      <td>1</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>3.026422</td>\n",
       "      <td>[[2.34000000e+02 2.24600000e+03 2.73880000e+04...</td>\n",
       "      <td>1.523363</td>\n",
       "      <td>54.870058</td>\n",
       "      <td>[[13.9300168   0.52264592]\\r\\n [ 0.52264592 24...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.562702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>66</td>\n",
       "      <td>2017.11.17</td>\n",
       "      <td>7200</td>\n",
       "      <td>25.15 s.tifheightmap.mat</td>\n",
       "      <td>4.064578</td>\n",
       "      <td>2445</td>\n",
       "      <td>4779.567013</td>\n",
       "      <td>0.909261</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575463</td>\n",
       "      <td>1.922593</td>\n",
       "      <td>[[2.48600000e+03 1.23125000e+05 7.47807700e+06...</td>\n",
       "      <td>-1.305940</td>\n",
       "      <td>284.521861</td>\n",
       "      <td>[[ 135.28370362 -122.912562  ]\\r\\n [-122.91256...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.472105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name        date  flow_rate                    source   peak_val  \\\n",
       "327  118_flipped  2017.11.22       7200  34.20 s.tifheightmap.mat   8.531226   \n",
       "296           73  2017.11.17       7200   5.65 s.tifheightmap.mat   8.775024   \n",
       "62     4_flipped  2017.11.09       1800     chip2_06heightmap.mat   4.629928   \n",
       "164   11_flipped  2017.11.09       3600     chip3_01heightmap.mat  14.033560   \n",
       "347           66  2017.11.17       7200  25.15 s.tifheightmap.mat   4.064578   \n",
       "\n",
       "      area        volume  eccentricity  euler_number    extent  \\\n",
       "327   3004  16022.993157      0.749848             1  0.661237   \n",
       "296  12462  54832.268548      0.984844             1  0.606778   \n",
       "62    1929   3566.980825      0.871295             1  0.562063   \n",
       "164    175    708.182643      0.665178             1  0.780000   \n",
       "347   2445   4779.567013      0.909261             1  0.575463   \n",
       "\n",
       "     mean_intensity                                            moments  \\\n",
       "327        5.333886  [[3.00400000e+03 1.05119000e+05 4.62409500e+06...   \n",
       "296        4.399957  [[1.24620000e+04 2.17597800e+06 4.54427324e+08...   \n",
       "62         1.849135  [[1.92900000e+03 6.37560000e+04 2.84693000e+06...   \n",
       "164        3.026422  [[2.34000000e+02 2.24600000e+03 2.73880000e+04...   \n",
       "347        1.922593  [[2.48600000e+03 1.23125000e+05 7.47807700e+06...   \n",
       "\n",
       "     orientation   perimeter  \\\n",
       "327    -1.031204  219.379726   \n",
       "296    -1.565982  721.019336   \n",
       "62      1.543902  209.095454   \n",
       "164     1.523363   54.870058   \n",
       "347    -1.305940  284.521861   \n",
       "\n",
       "                                        inertia_tensor  cluster  num_peaks  \\\n",
       "327  [[216.68422529 -91.61993795]\\r\\n [-91.61993795...        0          1   \n",
       "296  [[ 179.9360201   -27.90561956]\\r\\n [ -27.90561...        0          1   \n",
       "62   [[ 92.6177739    7.82965009]\\r\\n [  7.82965009...        1          1   \n",
       "164  [[13.9300168   0.52264592]\\r\\n [ 0.52264592 24...        2          1   \n",
       "347  [[ 135.28370362 -122.912562  ]\\r\\n [-122.91256...        1          1   \n",
       "\n",
       "     log_volume  \n",
       "327    9.681780  \n",
       "296   10.912034  \n",
       "62     8.179475  \n",
       "164    6.562702  \n",
       "347    8.472105  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(df.shape[0]/10)\n",
    "\n",
    "x_test = np.array(x[:test_size])\n",
    "y_test = df['flow_rate'][:test_size].values\n",
    "\n",
    "x_train = np.array(x[test_size:])\n",
    "y_train = df['flow_rate'][test_size:].values\n",
    "\n",
    "std = np.std(x_train,0)\n",
    "mean = np.mean(x_train,0)\n",
    "    \n",
    "x_train = (x_train-mean)/std\n",
    "x_test = (x_test-mean)/std        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_classes = 3\n",
    "nb_epoch = 30\n",
    "\n",
    "lmda = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train == 1800] = 0\n",
    "y_train[y_train == 3600] = 1\n",
    "y_train[y_train == 7200] = 2\n",
    "\n",
    "y_test[y_test == 1800] = 0\n",
    "y_test[y_test == 3600] = 1\n",
    "y_test[y_test == 7200] = 2\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes) \n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logistic_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim, input_dim=input_dim,\n",
    "                    kernel_regularizer=l2(lmda),\n",
    "                    activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model = build_logistic_model(input_dim, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 3)                 30        \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 314 samples, validate on 34 samples\n",
      "Epoch 1/30\n",
      "314/314 [==============================] - 3s 9ms/step - loss: 1.6213 - acc: 0.3217 - val_loss: 1.6689 - val_acc: 0.2353\n",
      "Epoch 2/30\n",
      "314/314 [==============================] - 0s 344us/step - loss: 1.5470 - acc: 0.3408 - val_loss: 1.5933 - val_acc: 0.2647\n",
      "Epoch 3/30\n",
      "314/314 [==============================] - 0s 353us/step - loss: 1.4878 - acc: 0.3408 - val_loss: 1.5312 - val_acc: 0.2647\n",
      "Epoch 4/30\n",
      "314/314 [==============================] - 0s 337us/step - loss: 1.4400 - acc: 0.3503 - val_loss: 1.4800 - val_acc: 0.2647\n",
      "Epoch 5/30\n",
      "314/314 [==============================] - 0s 328us/step - loss: 1.4012 - acc: 0.3408 - val_loss: 1.4375 - val_acc: 0.2941\n",
      "Epoch 6/30\n",
      "314/314 [==============================] - 0s 334us/step - loss: 1.3676 - acc: 0.3376 - val_loss: 1.4020 - val_acc: 0.2941\n",
      "Epoch 7/30\n",
      "314/314 [==============================] - 0s 347us/step - loss: 1.3401 - acc: 0.3535 - val_loss: 1.3703 - val_acc: 0.3529\n",
      "Epoch 8/30\n",
      "314/314 [==============================] - 0s 321us/step - loss: 1.3159 - acc: 0.3567 - val_loss: 1.3424 - val_acc: 0.3824\n",
      "Epoch 9/30\n",
      "314/314 [==============================] - 0s 328us/step - loss: 1.2943 - acc: 0.3503 - val_loss: 1.3192 - val_acc: 0.4118\n",
      "Epoch 10/30\n",
      "314/314 [==============================] - 0s 334us/step - loss: 1.2762 - acc: 0.3408 - val_loss: 1.2979 - val_acc: 0.4118\n",
      "Epoch 11/30\n",
      "314/314 [==============================] - 0s 356us/step - loss: 1.2589 - acc: 0.3439 - val_loss: 1.2779 - val_acc: 0.4118\n",
      "Epoch 12/30\n",
      "314/314 [==============================] - 0s 318us/step - loss: 1.2434 - acc: 0.3376 - val_loss: 1.2602 - val_acc: 0.4118\n",
      "Epoch 13/30\n",
      "314/314 [==============================] - 0s 331us/step - loss: 1.2298 - acc: 0.3376 - val_loss: 1.2442 - val_acc: 0.3824\n",
      "Epoch 14/30\n",
      "314/314 [==============================] - 0s 325us/step - loss: 1.2174 - acc: 0.3312 - val_loss: 1.2292 - val_acc: 0.3824\n",
      "Epoch 15/30\n",
      "314/314 [==============================] - 0s 321us/step - loss: 1.2058 - acc: 0.3312 - val_loss: 1.2165 - val_acc: 0.3824\n",
      "Epoch 16/30\n",
      "314/314 [==============================] - 0s 341us/step - loss: 1.1955 - acc: 0.3376 - val_loss: 1.2047 - val_acc: 0.4118\n",
      "Epoch 17/30\n",
      "314/314 [==============================] - 0s 350us/step - loss: 1.1867 - acc: 0.3503 - val_loss: 1.1932 - val_acc: 0.3824\n",
      "Epoch 18/30\n",
      "314/314 [==============================] - 0s 328us/step - loss: 1.1782 - acc: 0.3599 - val_loss: 1.1836 - val_acc: 0.3824\n",
      "Epoch 19/30\n",
      "314/314 [==============================] - 0s 356us/step - loss: 1.1703 - acc: 0.3631 - val_loss: 1.1744 - val_acc: 0.4118\n",
      "Epoch 20/30\n",
      "314/314 [==============================] - 0s 341us/step - loss: 1.1630 - acc: 0.3631 - val_loss: 1.1658 - val_acc: 0.3824\n",
      "Epoch 21/30\n",
      "314/314 [==============================] - 0s 315us/step - loss: 1.1561 - acc: 0.3631 - val_loss: 1.1577 - val_acc: 0.4118\n",
      "Epoch 22/30\n",
      "314/314 [==============================] - 0s 325us/step - loss: 1.1500 - acc: 0.3567 - val_loss: 1.1503 - val_acc: 0.4118\n",
      "Epoch 23/30\n",
      "314/314 [==============================] - 0s 328us/step - loss: 1.1441 - acc: 0.3599 - val_loss: 1.1435 - val_acc: 0.4118\n",
      "Epoch 24/30\n",
      "314/314 [==============================] - 0s 331us/step - loss: 1.1391 - acc: 0.3503 - val_loss: 1.1369 - val_acc: 0.3824\n",
      "Epoch 25/30\n",
      "314/314 [==============================] - 0s 318us/step - loss: 1.1343 - acc: 0.3471 - val_loss: 1.1308 - val_acc: 0.4118\n",
      "Epoch 26/30\n",
      "314/314 [==============================] - 0s 328us/step - loss: 1.1294 - acc: 0.3535 - val_loss: 1.1250 - val_acc: 0.4412\n",
      "Epoch 27/30\n",
      "314/314 [==============================] - 0s 347us/step - loss: 1.1257 - acc: 0.3599 - val_loss: 1.1200 - val_acc: 0.4118\n",
      "Epoch 28/30\n",
      "314/314 [==============================] - 0s 379us/step - loss: 1.1219 - acc: 0.3631 - val_loss: 1.1158 - val_acc: 0.4412\n",
      "Epoch 29/30\n",
      "314/314 [==============================] - 0s 344us/step - loss: 1.1184 - acc: 0.3790 - val_loss: 1.1120 - val_acc: 0.4412\n",
      "Epoch 30/30\n",
      "314/314 [==============================] - 0s 321us/step - loss: 1.1149 - acc: 0.3662 - val_loss: 1.1078 - val_acc: 0.4412\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size, epochs=nb_epoch,\n",
    "                    verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 1.107841505723841\n",
      "Test accuracy: 0.4411764705882353\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(x_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test,axis = 1) \n",
    "\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_classes = 3\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size, epochs=nb_epoch,\n",
    "                    verbose=1, validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 10\n",
      "Train on 312 samples, validate on 36 samples\n",
      "Epoch 1/20\n",
      "312/312 [==============================] - 0s 1ms/step - loss: 1.4575 - acc: 0.3494 - val_loss: 1.4218 - val_acc: 0.5278\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 0s 346us/step - loss: 1.3899 - acc: 0.3558 - val_loss: 1.3386 - val_acc: 0.5833\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 0s 436us/step - loss: 1.3398 - acc: 0.3526 - val_loss: 1.2824 - val_acc: 0.5833\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 0s 324us/step - loss: 1.3024 - acc: 0.3654 - val_loss: 1.2418 - val_acc: 0.5556\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 0s 336us/step - loss: 1.2736 - acc: 0.3750 - val_loss: 1.2142 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 0s 375us/step - loss: 1.2504 - acc: 0.3622 - val_loss: 1.1929 - val_acc: 0.4444\n",
      "Epoch 7/20\n",
      "312/312 [==============================] - 0s 327us/step - loss: 1.2322 - acc: 0.3718 - val_loss: 1.1770 - val_acc: 0.4444\n",
      "Epoch 8/20\n",
      "312/312 [==============================] - 0s 320us/step - loss: 1.2165 - acc: 0.3654 - val_loss: 1.1644 - val_acc: 0.4444\n",
      "Epoch 9/20\n",
      "312/312 [==============================] - 0s 340us/step - loss: 1.2036 - acc: 0.3750 - val_loss: 1.1540 - val_acc: 0.4444\n",
      "Epoch 10/20\n",
      "312/312 [==============================] - 0s 320us/step - loss: 1.1911 - acc: 0.3814 - val_loss: 1.1449 - val_acc: 0.4722\n",
      "Epoch 11/20\n",
      "312/312 [==============================] - 0s 324us/step - loss: 1.1815 - acc: 0.3910 - val_loss: 1.1380 - val_acc: 0.4167\n",
      "Epoch 12/20\n",
      "312/312 [==============================] - 0s 341us/step - loss: 1.1729 - acc: 0.4006 - val_loss: 1.1309 - val_acc: 0.4167\n",
      "Epoch 13/20\n",
      "312/312 [==============================] - 0s 320us/step - loss: 1.1652 - acc: 0.4006 - val_loss: 1.1246 - val_acc: 0.4167\n",
      "Epoch 14/20\n",
      "312/312 [==============================] - 0s 330us/step - loss: 1.1580 - acc: 0.4006 - val_loss: 1.1189 - val_acc: 0.4167\n",
      "Epoch 15/20\n",
      "312/312 [==============================] - 0s 330us/step - loss: 1.1522 - acc: 0.4006 - val_loss: 1.1139 - val_acc: 0.4167\n",
      "Epoch 16/20\n",
      "312/312 [==============================] - 0s 336us/step - loss: 1.1464 - acc: 0.4103 - val_loss: 1.1090 - val_acc: 0.4167\n",
      "Epoch 17/20\n",
      "312/312 [==============================] - 0s 324us/step - loss: 1.1412 - acc: 0.4103 - val_loss: 1.1053 - val_acc: 0.4167\n",
      "Epoch 18/20\n",
      "312/312 [==============================] - 0s 308us/step - loss: 1.1360 - acc: 0.4167 - val_loss: 1.1016 - val_acc: 0.3889\n",
      "Epoch 19/20\n",
      "312/312 [==============================] - 0s 330us/step - loss: 1.1316 - acc: 0.4071 - val_loss: 1.0982 - val_acc: 0.3889\n",
      "Epoch 20/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.1279 - acc: 0.4135 - val_loss: 1.0933 - val_acc: 0.3889\n",
      "Running Fold 2 / 10\n",
      "Train on 312 samples, validate on 36 samples\n",
      "Epoch 1/20\n",
      "312/312 [==============================] - 0s 1ms/step - loss: 1.6705 - acc: 0.2821 - val_loss: 1.3647 - val_acc: 0.3889\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.5722 - acc: 0.2917 - val_loss: 1.3002 - val_acc: 0.4167\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 0s 311us/step - loss: 1.4953 - acc: 0.3013 - val_loss: 1.2488 - val_acc: 0.4444\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.4309 - acc: 0.2853 - val_loss: 1.2120 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - ETA: 0s - loss: 1.3628 - acc: 0.317 - 0s 333us/step - loss: 1.3813 - acc: 0.2981 - val_loss: 1.1834 - val_acc: 0.5278\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 0s 340us/step - loss: 1.3404 - acc: 0.3045 - val_loss: 1.1602 - val_acc: 0.5556\n",
      "Epoch 7/20\n",
      "312/312 [==============================] - 0s 349us/step - loss: 1.3075 - acc: 0.3045 - val_loss: 1.1429 - val_acc: 0.5556\n",
      "Epoch 8/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.2795 - acc: 0.3173 - val_loss: 1.1293 - val_acc: 0.5556\n",
      "Epoch 9/20\n",
      "312/312 [==============================] - 0s 320us/step - loss: 1.2566 - acc: 0.3333 - val_loss: 1.1200 - val_acc: 0.5833\n",
      "Epoch 10/20\n",
      "312/312 [==============================] - ETA: 0s - loss: 1.2316 - acc: 0.349 - 0s 314us/step - loss: 1.2383 - acc: 0.3494 - val_loss: 1.1113 - val_acc: 0.6111\n",
      "Epoch 11/20\n",
      "312/312 [==============================] - ETA: 0s - loss: 1.2035 - acc: 0.385 - 0s 317us/step - loss: 1.2212 - acc: 0.3622 - val_loss: 1.1055 - val_acc: 0.6111\n",
      "Epoch 12/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.2067 - acc: 0.3750 - val_loss: 1.1001 - val_acc: 0.5833\n",
      "Epoch 13/20\n",
      "312/312 [==============================] - 0s 327us/step - loss: 1.1941 - acc: 0.3814 - val_loss: 1.0958 - val_acc: 0.5833\n",
      "Epoch 14/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.1836 - acc: 0.4006 - val_loss: 1.0940 - val_acc: 0.5833\n",
      "Epoch 15/20\n",
      "312/312 [==============================] - 0s 324us/step - loss: 1.1742 - acc: 0.4006 - val_loss: 1.0919 - val_acc: 0.5833\n",
      "Epoch 16/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.1653 - acc: 0.4135 - val_loss: 1.0902 - val_acc: 0.5556\n",
      "Epoch 17/20\n",
      "312/312 [==============================] - 0s 356us/step - loss: 1.1576 - acc: 0.4006 - val_loss: 1.0892 - val_acc: 0.5278\n",
      "Epoch 18/20\n",
      "312/312 [==============================] - 0s 320us/step - loss: 1.1505 - acc: 0.4103 - val_loss: 1.0879 - val_acc: 0.4722\n",
      "Epoch 19/20\n",
      "312/312 [==============================] - 0s 343us/step - loss: 1.1444 - acc: 0.4135 - val_loss: 1.0872 - val_acc: 0.4444\n",
      "Epoch 20/20\n",
      "312/312 [==============================] - 0s 330us/step - loss: 1.1388 - acc: 0.4167 - val_loss: 1.0874 - val_acc: 0.4444\n",
      "Running Fold 3 / 10\n",
      "Train on 312 samples, validate on 36 samples\n",
      "Epoch 1/20\n",
      "312/312 [==============================] - 0s 1ms/step - loss: 1.5458 - acc: 0.2885 - val_loss: 1.3868 - val_acc: 0.3056\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 0s 343us/step - loss: 1.4711 - acc: 0.3077 - val_loss: 1.3416 - val_acc: 0.3056\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 0s 308us/step - loss: 1.4095 - acc: 0.2949 - val_loss: 1.3099 - val_acc: 0.3333\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 0s 330us/step - loss: 1.3628 - acc: 0.3173 - val_loss: 1.2875 - val_acc: 0.3056\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.3272 - acc: 0.3269 - val_loss: 1.2697 - val_acc: 0.3056\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 0s 304us/step - loss: 1.2977 - acc: 0.3462 - val_loss: 1.2555 - val_acc: 0.3056\n",
      "Epoch 7/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.2735 - acc: 0.3622 - val_loss: 1.2425 - val_acc: 0.3333\n",
      "Epoch 8/20\n",
      "312/312 [==============================] - 0s 298us/step - loss: 1.2519 - acc: 0.3654 - val_loss: 1.2322 - val_acc: 0.3611\n",
      "Epoch 9/20\n",
      "312/312 [==============================] - 0s 311us/step - loss: 1.2336 - acc: 0.3718 - val_loss: 1.2226 - val_acc: 0.3611\n",
      "Epoch 10/20\n",
      "312/312 [==============================] - 0s 330us/step - loss: 1.2173 - acc: 0.3878 - val_loss: 1.2144 - val_acc: 0.3889\n",
      "Epoch 11/20\n",
      "312/312 [==============================] - 0s 304us/step - loss: 1.2030 - acc: 0.3974 - val_loss: 1.2065 - val_acc: 0.3889\n",
      "Epoch 12/20\n",
      "312/312 [==============================] - 0s 308us/step - loss: 1.1901 - acc: 0.3974 - val_loss: 1.2001 - val_acc: 0.3889\n",
      "Epoch 13/20\n",
      "312/312 [==============================] - ETA: 0s - loss: 1.1761 - acc: 0.408 - 0s 324us/step - loss: 1.1786 - acc: 0.4038 - val_loss: 1.1942 - val_acc: 0.3889\n",
      "Epoch 14/20\n",
      "312/312 [==============================] - 0s 298us/step - loss: 1.1681 - acc: 0.4103 - val_loss: 1.1892 - val_acc: 0.3333\n",
      "Epoch 15/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.1592 - acc: 0.4167 - val_loss: 1.1842 - val_acc: 0.3611\n",
      "Epoch 16/20\n",
      "312/312 [==============================] - 0s 324us/step - loss: 1.1507 - acc: 0.4167 - val_loss: 1.1785 - val_acc: 0.3611\n",
      "Epoch 17/20\n",
      "312/312 [==============================] - 0s 304us/step - loss: 1.1434 - acc: 0.4231 - val_loss: 1.1746 - val_acc: 0.3333\n",
      "Epoch 18/20\n",
      "312/312 [==============================] - 0s 324us/step - loss: 1.1362 - acc: 0.4295 - val_loss: 1.1711 - val_acc: 0.3333\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 0s 324us/step - loss: 1.1299 - acc: 0.4295 - val_loss: 1.1683 - val_acc: 0.3333\n",
      "Epoch 20/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.1240 - acc: 0.4263 - val_loss: 1.1651 - val_acc: 0.3333\n",
      "Running Fold 4 / 10\n",
      "Train on 312 samples, validate on 36 samples\n",
      "Epoch 1/20\n",
      "312/312 [==============================] - 0s 1ms/step - loss: 1.7147 - acc: 0.2821 - val_loss: 1.3861 - val_acc: 0.3611\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.5840 - acc: 0.2821 - val_loss: 1.3181 - val_acc: 0.3611\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 0s 311us/step - loss: 1.4805 - acc: 0.2788 - val_loss: 1.2659 - val_acc: 0.3611\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.3935 - acc: 0.2821 - val_loss: 1.2271 - val_acc: 0.3333\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 0s 327us/step - loss: 1.3272 - acc: 0.2853 - val_loss: 1.1998 - val_acc: 0.3333\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 0s 330us/step - loss: 1.2770 - acc: 0.2788 - val_loss: 1.1788 - val_acc: 0.3889\n",
      "Epoch 7/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.2360 - acc: 0.2756 - val_loss: 1.1636 - val_acc: 0.3889\n",
      "Epoch 8/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.2055 - acc: 0.2917 - val_loss: 1.1524 - val_acc: 0.4167\n",
      "Epoch 9/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.1811 - acc: 0.3173 - val_loss: 1.1433 - val_acc: 0.4167\n",
      "Epoch 10/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.1632 - acc: 0.3205 - val_loss: 1.1364 - val_acc: 0.4722\n",
      "Epoch 11/20\n",
      "312/312 [==============================] - 0s 304us/step - loss: 1.1497 - acc: 0.3462 - val_loss: 1.1310 - val_acc: 0.4722\n",
      "Epoch 12/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.1388 - acc: 0.3814 - val_loss: 1.1267 - val_acc: 0.4722\n",
      "Epoch 13/20\n",
      "312/312 [==============================] - 0s 298us/step - loss: 1.1304 - acc: 0.3846 - val_loss: 1.1222 - val_acc: 0.4722\n",
      "Epoch 14/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.1233 - acc: 0.4006 - val_loss: 1.1194 - val_acc: 0.4722\n",
      "Epoch 15/20\n",
      "312/312 [==============================] - 0s 324us/step - loss: 1.1180 - acc: 0.3974 - val_loss: 1.1160 - val_acc: 0.4444\n",
      "Epoch 16/20\n",
      "312/312 [==============================] - 0s 311us/step - loss: 1.1130 - acc: 0.4038 - val_loss: 1.1129 - val_acc: 0.4167\n",
      "Epoch 17/20\n",
      "312/312 [==============================] - 0s 327us/step - loss: 1.1085 - acc: 0.4006 - val_loss: 1.1104 - val_acc: 0.4167\n",
      "Epoch 18/20\n",
      "312/312 [==============================] - 0s 324us/step - loss: 1.1056 - acc: 0.4038 - val_loss: 1.1081 - val_acc: 0.3889\n",
      "Epoch 19/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.1018 - acc: 0.3910 - val_loss: 1.1056 - val_acc: 0.3889\n",
      "Epoch 20/20\n",
      "312/312 [==============================] - 0s 295us/step - loss: 1.0993 - acc: 0.4103 - val_loss: 1.1033 - val_acc: 0.3889\n",
      "Running Fold 5 / 10\n",
      "Train on 312 samples, validate on 36 samples\n",
      "Epoch 1/20\n",
      "312/312 [==============================] - 0s 1ms/step - loss: 1.6499 - acc: 0.3205 - val_loss: 1.5676 - val_acc: 0.2778\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 0s 311us/step - loss: 1.5970 - acc: 0.3173 - val_loss: 1.5170 - val_acc: 0.2778\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 0s 308us/step - loss: 1.5493 - acc: 0.3109 - val_loss: 1.4721 - val_acc: 0.2778\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.5078 - acc: 0.3205 - val_loss: 1.4321 - val_acc: 0.2778\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 0s 301us/step - loss: 1.4697 - acc: 0.3173 - val_loss: 1.3947 - val_acc: 0.2778\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.4349 - acc: 0.3205 - val_loss: 1.3622 - val_acc: 0.2778\n",
      "Epoch 7/20\n",
      "312/312 [==============================] - 0s 308us/step - loss: 1.4039 - acc: 0.3301 - val_loss: 1.3331 - val_acc: 0.2778\n",
      "Epoch 8/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.3759 - acc: 0.3429 - val_loss: 1.3073 - val_acc: 0.2778\n",
      "Epoch 9/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.3502 - acc: 0.3462 - val_loss: 1.2846 - val_acc: 0.2778\n",
      "Epoch 10/20\n",
      "312/312 [==============================] - 0s 343us/step - loss: 1.3278 - acc: 0.3526 - val_loss: 1.2651 - val_acc: 0.2778\n",
      "Epoch 11/20\n",
      "312/312 [==============================] - ETA: 0s - loss: 1.2934 - acc: 0.384 - 0s 314us/step - loss: 1.3069 - acc: 0.3590 - val_loss: 1.2472 - val_acc: 0.3056\n",
      "Epoch 12/20\n",
      "312/312 [==============================] - 0s 311us/step - loss: 1.2880 - acc: 0.3654 - val_loss: 1.2305 - val_acc: 0.3056\n",
      "Epoch 13/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.2709 - acc: 0.3686 - val_loss: 1.2156 - val_acc: 0.3056\n",
      "Epoch 14/20\n",
      "312/312 [==============================] - ETA: 0s - loss: 1.2574 - acc: 0.365 - 0s 320us/step - loss: 1.2554 - acc: 0.3718 - val_loss: 1.2035 - val_acc: 0.3056\n",
      "Epoch 15/20\n",
      "312/312 [==============================] - 0s 308us/step - loss: 1.2415 - acc: 0.3814 - val_loss: 1.1921 - val_acc: 0.3056\n",
      "Epoch 16/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.2291 - acc: 0.3878 - val_loss: 1.1817 - val_acc: 0.3056\n",
      "Epoch 17/20\n",
      "312/312 [==============================] - 0s 330us/step - loss: 1.2171 - acc: 0.3782 - val_loss: 1.1723 - val_acc: 0.3611\n",
      "Epoch 18/20\n",
      "312/312 [==============================] - 0s 317us/step - loss: 1.2070 - acc: 0.3910 - val_loss: 1.1635 - val_acc: 0.3889\n",
      "Epoch 19/20\n",
      "312/312 [==============================] - 0s 304us/step - loss: 1.1971 - acc: 0.4006 - val_loss: 1.1557 - val_acc: 0.3889\n",
      "Epoch 20/20\n",
      "312/312 [==============================] - 0s 320us/step - loss: 1.1881 - acc: 0.4071 - val_loss: 1.1487 - val_acc: 0.4167\n",
      "Running Fold 6 / 10\n",
      "Train on 312 samples, validate on 36 samples\n",
      "Epoch 1/20\n",
      "312/312 [==============================] - 0s 1ms/step - loss: 1.8509 - acc: 0.3462 - val_loss: 1.9610 - val_acc: 0.3611\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 0s 324us/step - loss: 1.7378 - acc: 0.3462 - val_loss: 1.8444 - val_acc: 0.3611\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.6399 - acc: 0.3429 - val_loss: 1.7440 - val_acc: 0.3611\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 0s 330us/step - loss: 1.5575 - acc: 0.3494 - val_loss: 1.6550 - val_acc: 0.3611\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.4869 - acc: 0.3526 - val_loss: 1.5793 - val_acc: 0.3889\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 0s 343us/step - loss: 1.4270 - acc: 0.3462 - val_loss: 1.5131 - val_acc: 0.3889\n",
      "Epoch 7/20\n",
      "312/312 [==============================] - 0s 311us/step - loss: 1.3776 - acc: 0.3622 - val_loss: 1.4555 - val_acc: 0.3889\n",
      "Epoch 8/20\n",
      "312/312 [==============================] - 0s 320us/step - loss: 1.3363 - acc: 0.3686 - val_loss: 1.4078 - val_acc: 0.3333\n",
      "Epoch 9/20\n",
      "312/312 [==============================] - 0s 320us/step - loss: 1.3027 - acc: 0.3718 - val_loss: 1.3650 - val_acc: 0.3333\n",
      "Epoch 10/20\n",
      "312/312 [==============================] - ETA: 0s - loss: 1.2735 - acc: 0.363 - 0s 330us/step - loss: 1.2740 - acc: 0.3686 - val_loss: 1.3302 - val_acc: 0.3611\n",
      "Epoch 11/20\n",
      "312/312 [==============================] - 0s 320us/step - loss: 1.2506 - acc: 0.3814 - val_loss: 1.3022 - val_acc: 0.3056\n",
      "Epoch 12/20\n",
      "312/312 [==============================] - 0s 311us/step - loss: 1.2312 - acc: 0.3814 - val_loss: 1.2815 - val_acc: 0.3056\n",
      "Epoch 13/20\n",
      "312/312 [==============================] - 0s 320us/step - loss: 1.2148 - acc: 0.4006 - val_loss: 1.2616 - val_acc: 0.3056\n",
      "Epoch 14/20\n",
      "312/312 [==============================] - 0s 327us/step - loss: 1.2003 - acc: 0.4006 - val_loss: 1.2454 - val_acc: 0.3056\n",
      "Epoch 15/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.1871 - acc: 0.4006 - val_loss: 1.2304 - val_acc: 0.3333\n",
      "Epoch 16/20\n",
      "312/312 [==============================] - ETA: 0s - loss: 1.1884 - acc: 0.384 - 0s 314us/step - loss: 1.1766 - acc: 0.3910 - val_loss: 1.2190 - val_acc: 0.3611\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 0s 327us/step - loss: 1.1665 - acc: 0.4006 - val_loss: 1.2082 - val_acc: 0.3611\n",
      "Epoch 18/20\n",
      "312/312 [==============================] - 0s 327us/step - loss: 1.1584 - acc: 0.3974 - val_loss: 1.1980 - val_acc: 0.3611\n",
      "Epoch 19/20\n",
      "312/312 [==============================] - ETA: 0s - loss: 1.1436 - acc: 0.408 - 0s 304us/step - loss: 1.1506 - acc: 0.4006 - val_loss: 1.1886 - val_acc: 0.3611\n",
      "Epoch 20/20\n",
      "312/312 [==============================] - 0s 314us/step - loss: 1.1440 - acc: 0.3910 - val_loss: 1.1784 - val_acc: 0.3333\n",
      "Running Fold 7 / 10\n",
      "Train on 315 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 1.6859 - acc: 0.3492 - val_loss: 1.7179 - val_acc: 0.4545\n",
      "Epoch 2/20\n",
      "315/315 [==============================] - 0s 416us/step - loss: 1.5943 - acc: 0.3492 - val_loss: 1.6133 - val_acc: 0.4545\n",
      "Epoch 3/20\n",
      "315/315 [==============================] - 0s 324us/step - loss: 1.5141 - acc: 0.3460 - val_loss: 1.5226 - val_acc: 0.4545\n",
      "Epoch 4/20\n",
      "315/315 [==============================] - 0s 298us/step - loss: 1.4455 - acc: 0.3556 - val_loss: 1.4466 - val_acc: 0.4545\n",
      "Epoch 5/20\n",
      "315/315 [==============================] - 0s 333us/step - loss: 1.3889 - acc: 0.3587 - val_loss: 1.3834 - val_acc: 0.4545\n",
      "Epoch 6/20\n",
      "315/315 [==============================] - 0s 308us/step - loss: 1.3421 - acc: 0.3619 - val_loss: 1.3317 - val_acc: 0.4545\n",
      "Epoch 7/20\n",
      "315/315 [==============================] - 0s 311us/step - loss: 1.3034 - acc: 0.3810 - val_loss: 1.2900 - val_acc: 0.4545\n",
      "Epoch 8/20\n",
      "315/315 [==============================] - 0s 311us/step - loss: 1.2723 - acc: 0.3746 - val_loss: 1.2574 - val_acc: 0.4545\n",
      "Epoch 9/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.2471 - acc: 0.3778 - val_loss: 1.2316 - val_acc: 0.4242\n",
      "Epoch 10/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.2263 - acc: 0.3810 - val_loss: 1.2120 - val_acc: 0.3939\n",
      "Epoch 11/20\n",
      "315/315 [==============================] - 0s 308us/step - loss: 1.2094 - acc: 0.3873 - val_loss: 1.1964 - val_acc: 0.3939\n",
      "Epoch 12/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.1955 - acc: 0.4032 - val_loss: 1.1838 - val_acc: 0.3939\n",
      "Epoch 13/20\n",
      "315/315 [==============================] - 0s 305us/step - loss: 1.1834 - acc: 0.4095 - val_loss: 1.1736 - val_acc: 0.3939\n",
      "Epoch 14/20\n",
      "315/315 [==============================] - 0s 301us/step - loss: 1.1730 - acc: 0.4063 - val_loss: 1.1647 - val_acc: 0.3939\n",
      "Epoch 15/20\n",
      "315/315 [==============================] - 0s 317us/step - loss: 1.1641 - acc: 0.4127 - val_loss: 1.1570 - val_acc: 0.3939\n",
      "Epoch 16/20\n",
      "315/315 [==============================] - 0s 311us/step - loss: 1.1561 - acc: 0.4063 - val_loss: 1.1502 - val_acc: 0.4242\n",
      "Epoch 17/20\n",
      "315/315 [==============================] - 0s 295us/step - loss: 1.1487 - acc: 0.4032 - val_loss: 1.1442 - val_acc: 0.3939\n",
      "Epoch 18/20\n",
      "315/315 [==============================] - 0s 324us/step - loss: 1.1424 - acc: 0.3968 - val_loss: 1.1388 - val_acc: 0.3939\n",
      "Epoch 19/20\n",
      "315/315 [==============================] - 0s 339us/step - loss: 1.1364 - acc: 0.4063 - val_loss: 1.1334 - val_acc: 0.3333\n",
      "Epoch 20/20\n",
      "315/315 [==============================] - 0s 298us/step - loss: 1.1311 - acc: 0.3968 - val_loss: 1.1284 - val_acc: 0.3333\n",
      "Running Fold 8 / 10\n",
      "Train on 315 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 1.4973 - acc: 0.3492 - val_loss: 1.4319 - val_acc: 0.3939\n",
      "Epoch 2/20\n",
      "315/315 [==============================] - 0s 476us/step - loss: 1.4414 - acc: 0.3460 - val_loss: 1.3880 - val_acc: 0.4242\n",
      "Epoch 3/20\n",
      "315/315 [==============================] - 0s 416us/step - loss: 1.3944 - acc: 0.3429 - val_loss: 1.3513 - val_acc: 0.4242\n",
      "Epoch 4/20\n",
      "315/315 [==============================] - 0s 330us/step - loss: 1.3544 - acc: 0.3587 - val_loss: 1.3201 - val_acc: 0.4545\n",
      "Epoch 5/20\n",
      "315/315 [==============================] - 0s 327us/step - loss: 1.3205 - acc: 0.3556 - val_loss: 1.2928 - val_acc: 0.4545\n",
      "Epoch 6/20\n",
      "315/315 [==============================] - 0s 324us/step - loss: 1.2916 - acc: 0.3683 - val_loss: 1.2696 - val_acc: 0.4545\n",
      "Epoch 7/20\n",
      "315/315 [==============================] - 0s 317us/step - loss: 1.2667 - acc: 0.3714 - val_loss: 1.2486 - val_acc: 0.4545\n",
      "Epoch 8/20\n",
      "315/315 [==============================] - 0s 339us/step - loss: 1.2450 - acc: 0.3746 - val_loss: 1.2296 - val_acc: 0.4545\n",
      "Epoch 9/20\n",
      "315/315 [==============================] - 0s 305us/step - loss: 1.2261 - acc: 0.4000 - val_loss: 1.2136 - val_acc: 0.4545\n",
      "Epoch 10/20\n",
      "315/315 [==============================] - 0s 327us/step - loss: 1.2101 - acc: 0.4032 - val_loss: 1.1987 - val_acc: 0.4545\n",
      "Epoch 11/20\n",
      "315/315 [==============================] - 0s 320us/step - loss: 1.1960 - acc: 0.4063 - val_loss: 1.1860 - val_acc: 0.4242\n",
      "Epoch 12/20\n",
      "315/315 [==============================] - 0s 324us/step - loss: 1.1833 - acc: 0.4063 - val_loss: 1.1752 - val_acc: 0.4242\n",
      "Epoch 13/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.1723 - acc: 0.4032 - val_loss: 1.1661 - val_acc: 0.4242\n",
      "Epoch 14/20\n",
      "315/315 [==============================] - 0s 311us/step - loss: 1.1628 - acc: 0.4127 - val_loss: 1.1568 - val_acc: 0.4242\n",
      "Epoch 15/20\n",
      "315/315 [==============================] - 0s 330us/step - loss: 1.1540 - acc: 0.4127 - val_loss: 1.1482 - val_acc: 0.4242\n",
      "Epoch 16/20\n",
      "315/315 [==============================] - 0s 336us/step - loss: 1.1460 - acc: 0.4095 - val_loss: 1.1407 - val_acc: 0.3939\n",
      "Epoch 17/20\n",
      "315/315 [==============================] - 0s 320us/step - loss: 1.1389 - acc: 0.4095 - val_loss: 1.1343 - val_acc: 0.4242\n",
      "Epoch 18/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.1332 - acc: 0.4159 - val_loss: 1.1285 - val_acc: 0.3939\n",
      "Epoch 19/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.1272 - acc: 0.4286 - val_loss: 1.1233 - val_acc: 0.4545\n",
      "Epoch 20/20\n",
      "315/315 [==============================] - 0s 311us/step - loss: 1.1218 - acc: 0.4286 - val_loss: 1.1181 - val_acc: 0.4545\n",
      "Running Fold 9 / 10\n",
      "Train on 315 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 1.6933 - acc: 0.2984 - val_loss: 1.6609 - val_acc: 0.3030\n",
      "Epoch 2/20\n",
      "315/315 [==============================] - 0s 311us/step - loss: 1.5653 - acc: 0.3143 - val_loss: 1.5546 - val_acc: 0.3030\n",
      "Epoch 3/20\n",
      "315/315 [==============================] - 0s 333us/step - loss: 1.4617 - acc: 0.3143 - val_loss: 1.4700 - val_acc: 0.3333\n",
      "Epoch 4/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.3816 - acc: 0.3429 - val_loss: 1.4065 - val_acc: 0.3333\n",
      "Epoch 5/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.3218 - acc: 0.3587 - val_loss: 1.3583 - val_acc: 0.3333\n",
      "Epoch 6/20\n",
      "315/315 [==============================] - 0s 320us/step - loss: 1.2793 - acc: 0.3683 - val_loss: 1.3228 - val_acc: 0.3333\n",
      "Epoch 7/20\n",
      "315/315 [==============================] - 0s 327us/step - loss: 1.2484 - acc: 0.3778 - val_loss: 1.2963 - val_acc: 0.3030\n",
      "Epoch 8/20\n",
      "315/315 [==============================] - 0s 327us/step - loss: 1.2260 - acc: 0.3683 - val_loss: 1.2757 - val_acc: 0.3333\n",
      "Epoch 9/20\n",
      "315/315 [==============================] - 0s 333us/step - loss: 1.2090 - acc: 0.3651 - val_loss: 1.2596 - val_acc: 0.3939\n",
      "Epoch 10/20\n",
      "315/315 [==============================] - 0s 308us/step - loss: 1.1955 - acc: 0.3683 - val_loss: 1.2465 - val_acc: 0.3636\n",
      "Epoch 11/20\n",
      "315/315 [==============================] - 0s 308us/step - loss: 1.1845 - acc: 0.3714 - val_loss: 1.2350 - val_acc: 0.3636\n",
      "Epoch 12/20\n",
      "315/315 [==============================] - 0s 320us/step - loss: 1.1756 - acc: 0.3841 - val_loss: 1.2260 - val_acc: 0.3636\n",
      "Epoch 13/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.1675 - acc: 0.3905 - val_loss: 1.2177 - val_acc: 0.3636\n",
      "Epoch 14/20\n",
      "315/315 [==============================] - 0s 327us/step - loss: 1.1606 - acc: 0.4000 - val_loss: 1.2107 - val_acc: 0.3939\n",
      "Epoch 15/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.1543 - acc: 0.3873 - val_loss: 1.2041 - val_acc: 0.3939\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/315 [==============================] - 0s 355us/step - loss: 1.1480 - acc: 0.3873 - val_loss: 1.1990 - val_acc: 0.3939\n",
      "Epoch 17/20\n",
      "315/315 [==============================] - 0s 333us/step - loss: 1.1429 - acc: 0.3937 - val_loss: 1.1942 - val_acc: 0.3939\n",
      "Epoch 18/20\n",
      "315/315 [==============================] - 0s 327us/step - loss: 1.1382 - acc: 0.3937 - val_loss: 1.1897 - val_acc: 0.3939\n",
      "Epoch 19/20\n",
      "315/315 [==============================] - 0s 336us/step - loss: 1.1335 - acc: 0.3968 - val_loss: 1.1858 - val_acc: 0.3939\n",
      "Epoch 20/20\n",
      "315/315 [==============================] - 0s 301us/step - loss: 1.1294 - acc: 0.3937 - val_loss: 1.1815 - val_acc: 0.3939\n",
      "Running Fold 10 / 10\n",
      "Train on 315 samples, validate on 33 samples\n",
      "Epoch 1/20\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 1.6318 - acc: 0.3397 - val_loss: 1.1990 - val_acc: 0.4545\n",
      "Epoch 2/20\n",
      "315/315 [==============================] - 0s 330us/step - loss: 1.5408 - acc: 0.3460 - val_loss: 1.1794 - val_acc: 0.4545\n",
      "Epoch 3/20\n",
      "315/315 [==============================] - 0s 308us/step - loss: 1.4617 - acc: 0.3429 - val_loss: 1.1671 - val_acc: 0.4545\n",
      "Epoch 4/20\n",
      "315/315 [==============================] - 0s 320us/step - loss: 1.3966 - acc: 0.3429 - val_loss: 1.1597 - val_acc: 0.4242\n",
      "Epoch 5/20\n",
      "315/315 [==============================] - 0s 324us/step - loss: 1.3413 - acc: 0.3460 - val_loss: 1.1561 - val_acc: 0.4242\n",
      "Epoch 6/20\n",
      "315/315 [==============================] - 0s 308us/step - loss: 1.2950 - acc: 0.3492 - val_loss: 1.1552 - val_acc: 0.3939\n",
      "Epoch 7/20\n",
      "315/315 [==============================] - 0s 317us/step - loss: 1.2585 - acc: 0.3460 - val_loss: 1.1563 - val_acc: 0.4545\n",
      "Epoch 8/20\n",
      "315/315 [==============================] - 0s 317us/step - loss: 1.2283 - acc: 0.3556 - val_loss: 1.1579 - val_acc: 0.4242\n",
      "Epoch 9/20\n",
      "315/315 [==============================] - 0s 311us/step - loss: 1.2045 - acc: 0.3492 - val_loss: 1.1595 - val_acc: 0.4242\n",
      "Epoch 10/20\n",
      "315/315 [==============================] - 0s 317us/step - loss: 1.1856 - acc: 0.3460 - val_loss: 1.1617 - val_acc: 0.4242\n",
      "Epoch 11/20\n",
      "315/315 [==============================] - 0s 324us/step - loss: 1.1701 - acc: 0.3651 - val_loss: 1.1627 - val_acc: 0.4242\n",
      "Epoch 12/20\n",
      "315/315 [==============================] - 0s 320us/step - loss: 1.1582 - acc: 0.3714 - val_loss: 1.1634 - val_acc: 0.3939\n",
      "Epoch 13/20\n",
      "315/315 [==============================] - 0s 330us/step - loss: 1.1478 - acc: 0.3873 - val_loss: 1.1627 - val_acc: 0.3636\n",
      "Epoch 14/20\n",
      "315/315 [==============================] - 0s 327us/step - loss: 1.1396 - acc: 0.3841 - val_loss: 1.1614 - val_acc: 0.3333\n",
      "Epoch 15/20\n",
      "315/315 [==============================] - 0s 317us/step - loss: 1.1322 - acc: 0.4032 - val_loss: 1.1601 - val_acc: 0.3030\n",
      "Epoch 16/20\n",
      "315/315 [==============================] - 0s 317us/step - loss: 1.1258 - acc: 0.4032 - val_loss: 1.1586 - val_acc: 0.3333\n",
      "Epoch 17/20\n",
      "315/315 [==============================] - 0s 317us/step - loss: 1.1203 - acc: 0.4222 - val_loss: 1.1579 - val_acc: 0.3636\n",
      "Epoch 18/20\n",
      "315/315 [==============================] - 0s 314us/step - loss: 1.1149 - acc: 0.4159 - val_loss: 1.1560 - val_acc: 0.3333\n",
      "Epoch 19/20\n",
      "315/315 [==============================] - 0s 349us/step - loss: 1.1106 - acc: 0.4127 - val_loss: 1.1546 - val_acc: 0.3333\n",
      "Epoch 20/20\n",
      "315/315 [==============================] - 0s 305us/step - loss: 1.1061 - acc: 0.4317 - val_loss: 1.1525 - val_acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "n_folds = 10\n",
    "\n",
    "labels = df['flow_rate'].values\n",
    "data = x\n",
    "\n",
    "labels[labels == 1800] = 0\n",
    "labels[labels == 3600] = 1\n",
    "labels[labels == 7200] = 2\n",
    "\n",
    "labels = np_utils.to_categorical(labels, nb_classes) \n",
    "\n",
    "skf = StratifiedKFold(df['flow_rate'].values, n_folds=n_folds, shuffle=True)\n",
    "avg_acc = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "    model = None # Clearing the NN.\n",
    "    model = build_logistic_model(input_dim, nb_classes)\n",
    "    \n",
    "    std = np.std(data[train],0)\n",
    "    mean = np.mean(data[train],0)\n",
    "    \n",
    "    x_train = (data[train]-mean)/std\n",
    "    x_test = (data[test] - mean)/std        \n",
    "    \n",
    "    avg_acc += train_and_evaluate_model(model, x_train, labels[train], x_test, labels[test])\n",
    "    \n",
    "    # Predict the values from the validation dataset\n",
    "    Y_pred = model.predict(x_test)\n",
    "    # Convert predictions classes to one hot vectors \n",
    "    Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "    # Convert validation observations to one hot vectors\n",
    "    Y_true = np.argmax(labels[test],axis = 1) \n",
    "    \n",
    "    y_true.extend(Y_true)\n",
    "    y_pred.extend(Y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  0.3820707070707071\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Accuracy: \", avg_acc/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(np.array(y_true), np.array(y_pred)) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(3)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
